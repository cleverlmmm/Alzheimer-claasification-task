{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12107936,"sourceType":"datasetVersion","datasetId":7623043},{"sourceId":13122993,"sourceType":"datasetVersion","datasetId":8312961}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"copy from Continue_Numer1_EI_MCMC ver1\nver1---------目的是得到35次纯随机搜索的日志以及对应的可视化图\nver2---------ver1运行超时，ver2继续直到35次随机探索完成\n\n================\ncopy from the EI_MCMC_random, and this new is set to keep on running the random exploration\n\nver1,ver2: 参数epoch=12,忘记改成40了，此版本输出是无效数据\nver3: epoch改成40，要和EI_MCMC_BO中的设定一致","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\n\n\n# import GPyOpt\nimport time\nimport shutil\nimport pathlib\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\n\n\nfrom tensorflow import keras\nfrom keras import models, optimizers, metrics, layers, regularizers, losses\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad\nimport json\n\ndef set_reproducibility(seed=88):\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.config.experimental.enable_op_determinism()\n    \n    print(f\"✅ 全局随机种子已设置为: {seed}，并已启用TensorFlow确定性操作。\")\n\n\nSEED_VALUE = 88 \nset_reproducibility(SEED_VALUE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:04.043749Z","iopub.execute_input":"2025-09-08T08:06:04.043969Z","iopub.status.idle":"2025-09-08T08:06:21.139938Z","shell.execute_reply.started":"2025-09-08T08:06:04.043947Z","shell.execute_reply":"2025-09-08T08:06:21.138941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom tensorflow import keras\nfrom keras import layers\nimport matplotlib.image as img\n\n\n'''加载到output路径并划分文件夹'''\n\n!pip install split-folders -q\nimport splitfolders\ninput_dir=\"/kaggle/input/augmented-alzheimer-data/augmented_data\"\noutput_dir=\"./output\"\nsplitfolders.ratio(\n    input_dir, \n    output=output_dir, \n    seed=1345, \n    ratio=(.70, 0.15,0.15),\n    group_prefix=None \n) \nprint(\"Dataset split completed!\")\n\n#验证输出路径文件夹文件数量\ndef count_files(directory):\n    for split in ['train', 'val', 'test']:\n        split_path = os.path.join(directory, split)\n        print(f\"\\n{split.upper()} set:\")\n        for class_name in os.listdir(split_path):\n            class_path = os.path.join(split_path, class_name)\n            num_files = len(os.listdir(class_path))\n            print(f\"  {class_name}: {num_files} images\")\n\ncount_files(\"./output\")\n\nAUTOTUNE = tf.data.AUTOTUNE\n'''DATALOAD\n从划分好的数据集子目录train..加载数据，转换为 TensorFlow 的 Dataset 对象，便于后续训练'''\nIMG_HEIGHT = 224\nIMG_WIDTH = 224 \nbatch_size=32\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/train\",\nseed=123,\nlabel_mode='int',\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=True\n).prefetch(buffer_size=AUTOTUNE)\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/test\",\nlabel_mode='int',\nseed=123,\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=False  \n).prefetch(buffer_size=AUTOTUNE)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/val\",\nlabel_mode='int',\nseed=123,\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=False\n).prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:06:21.14155Z","iopub.execute_input":"2025-09-08T08:06:21.14208Z","iopub.status.idle":"2025-09-08T08:07:43.963258Z","shell.execute_reply.started":"2025-09-08T08:06:21.142059Z","shell.execute_reply":"2025-09-08T08:07:43.962537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, time, gc, json\nimport numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nimport tensorflow as tf, tensorflow_probability as tfp\nfrom tensorflow import keras\nfrom keras import layers, regularizers\nimport scipy.stats as stats\ntfd = tfp.distributions\nsns.set_style('whitegrid')\ntf.keras.backend.set_floatx('float64')\n\n\ndef build_pretrained_model(hparams):\n    # 1. 加载预训练的EfficientNet，不包含顶部的分类层\n    base_model = tf.keras.applications.EfficientNetB0(\n        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    # 2. 冻结预训练模型的权重\n    base_model.trainable = False\n    \n    # 3. 确定从哪个块开始解冻（微调）\n    fine_tune_at_block = 'block5a_expand_conv'\n    # 3. 遍历所有层，解冻 fine_tune_at_block 及之后的所有层\n    set_trainable = False\n    for layer in base_model.layers:\n        if layer.name == fine_tune_at_block:\n            set_trainable = True\n        if set_trainable:\n            layer.trainable = True\n        else:\n            layer.trainable = False\n\n    for layer in base_model.layers:\n        if isinstance(layer, layers.BatchNormalization):\n            layer.trainable = False\n    \n\n    dense_units = int(hparams.get('dense_units', 128))\n    dropout_rate = hparams.get('dropout_rate', 0.5)\n    l2_reg = hparams.get('l2_reg', 0.001)\n    lr = hparams.get('lr', 0.0001)\n    optimizer_choice = int(hparams.get('optimizer_choice', 0))\n    momentum = hparams.get('momentum', 0.9)\n    \n    \n    model = tf.keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(), # 使用全局平均池化替代Flatten，参数更少，不易过拟合\n        layers.Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)), # 使用更小的L2值\n        layers.Dropout(dropout_rate),\n        layers.Dense(4, activation='softmax') # 4个类别\n    ])\n\n    if optimizer_choice == 0:\n        print(\"--- 使用 Adam 优化器 ---\")\n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    elif optimizer_choice == 1:\n        print(f\"--- 使用 SGD 优化器 (momentum={momentum:.3f}) ---\")\n        optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n    elif optimizer_choice == 2:\n        print(\"--- 使用 RMSprop 优化器 ---\")\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n    elif optimizer_choice ==3:\n        print(\"--- 使用 Adagrad 优化器 ---\")\n        optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n    else: # optimizer_choice == 分数\n        print(\"--- 无法找到对应的优化器，请检查optimizer_choice value ---\")\n\n    \n    model.compile(\n        optimizer=optimizer,\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n   \n    return model\n\n\nclass TrainingCallback(keras.callbacks.Callback):\n    def __init__(self, epochs):\n        super().__init__()\n        self.epochs = epochs\n    def on_train_begin(self, logs=None):\n        header = \"{0:^8s}{1:^11s}{2:^11s}{3:^12s}{4:^12s}{5:^12s}\".format('Epoch', 'Loss', 'Acc', 'Val_Loss', 'Val_Acc', 'Duration')\n        print(header)\n    def on_epoch_begin(self, epoch, logs=None): \n        self.ep_start = time.time()\n    def on_epoch_end(self, epoch, logs=None):\n        duration = time.time() - self.ep_start\n        acc = logs.get('accuracy', 0); val_acc = logs.get('val_accuracy', 0)\n        loss = logs.get('loss', 0); val_loss = logs.get('val_loss', 0)\n        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^10.3f}{acc * 100:^11.3f}{val_loss:^12.5f}{val_acc * 100:^12.3f}{duration:^12.2f}'\n        print(msg)\n        \n\n\ndef train_and_evaluate_model(hparams,train_ds,val_ds,epochs=30):\n    model = build_pretrained_model(hparams) \n    \n\n    training_callback = TrainingCallback(\n        epochs=epochs\n    )\n    \n    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',\n        patience=5,  \n        min_delta=0.005,\n        restore_best_weights=True, \n        verbose=0 \n    )\n    \n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        verbose=0,\n        callbacks=[training_callback,early_stopping_callback]\n    )\n\n    loss, accuracy = model.evaluate(val_ds, verbose=0)\n    best_epoch_num = early_stopping_callback.best_epoch + 1\n    print(f\"  > 训练评估完成: Best Val Acc: {accuracy:.4f}, 对应的 Loss:{loss:.4f}(at epoch {best_epoch_num})\")\n    \n    return accuracy\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:52:50.375235Z","iopub.execute_input":"2025-09-08T08:52:50.375984Z","iopub.status.idle":"2025-09-08T08:52:50.412874Z","shell.execute_reply.started":"2025-09-08T08:52:50.37596Z","shell.execute_reply":"2025-09-08T08:52:50.412172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BayesianOptimizer:\n    def __init__(self, search_space, init_points=5, exploration=0.01, \n                 log_file=\"custom_bo_log.json\", initial_X=None, initial_y=None):\n        self.search_space = np.array(search_space)\n        self.dim = len(search_space)\n        self.init_points = init_points\n        self.exploration = exploration\n        self.log_file = log_file\n        # --- 新增逻辑: 加载预先计算的数据 ---\n        if initial_X is not None and initial_y is not None:\n            self.X_obs = initial_X\n            self.y_obs = initial_y\n            print(f\"优化器已使用 {len(self.y_obs)} 个预加载点进行初始化。\")\n            # 2b. 日志回填 (现在可以安全地访问 self.log_file)\n            print(f\"正在创建新的日志文件 '{self.log_file}' 并回填历史数据...\")\n            if os.path.exists(self.log_file): \n                os.remove(self.log_file)\n\n            for i in range(len(self.y_obs)):\n                self._write_log(iteration=i + 1, x_new=self.X_obs[i], y_new=self.y_obs[i])\n            print(f\"历史数据回填完成。共写入 {len(self.y_obs)} 条记录。\")\n        else:\n            self.X_obs = np.zeros((0, self.dim))\n            self.y_obs = np.zeros(0)\n            if os.path.exists(self.log_file):\n                os.remove(self.log_file)\n            \n        \n        self.theta_dim = self.dim + 2 # length_scales (dim) + amplitude + noise\n        \n        self.num_samples = 20\n        self.burn_in = 100\n        \n        \n    def random_sample(self):\n        \"\"\"在搜索空间内随机采样一个超参数点。\"\"\"\n        return np.array([np.random.uniform(low, high) for (low, high) in self.search_space])\n\n    @tf.function(jit_compile=True) \n    def matern52_kernel(self, X1, X2, theta):\n        X1_tf = tf.cast(X1, tf.float64)\n        X2_tf = tf.cast(X2, tf.float64)\n        theta = tf.cast(theta, tf.float64)\n        \n        length_scales = theta[:self.dim]\n        amplitude = theta[self.dim]\n        \n        X1_scaled = X1_tf / length_scales\n        X2_scaled = X2_tf / length_scales\n\n        r2 = tf.reduce_sum(tf.square(X1_scaled), axis=1, keepdims=True) - 2 * tf.matmul(X1_scaled, X2_scaled, transpose_b=True) + tf.reduce_sum(tf.square(X2_scaled), axis=1)\n        # 确保 r2 不为负数\n        r2 = tf.maximum(r2, 1e-12)\n        r = tf.sqrt(r2)\n        \n        five_64 = tf.constant(5.0, dtype=tf.float64)\n        three_64 = tf.constant(3.0, dtype=tf.float64)\n        one_64 = tf.constant(1.0, dtype=tf.float64)\n        \n        sqrt5_64 = tf.sqrt(five_64)\n        term = (one_64 + sqrt5_64 * r + (five_64 / three_64) * r2) * tf.exp(-sqrt5_64 * r)\n        \n        K = amplitude**2 * term\n        return K\n\n    \n    @tf.function(jit_compile=True) \n    def log_posterior(self, theta, X, y):\n        f64 = tf.constant(1e-6, dtype=tf.float64)\n        two_pi_64 = tf.constant(2.0 * np.pi, dtype=tf.float64)\n        half_64 = tf.constant(0.5, dtype=tf.float64)\n        \n        X_tf = tf.cast(X, dtype=tf.float64)\n        y_tf = tf.cast(y, dtype=tf.float64)\n        theta_tf = tf.cast(theta, dtype=tf.float64)\n        \n        noise = theta_tf[self.dim + 1]**2\n        \n        K = self.matern52_kernel(X_tf, X_tf, theta_tf)\n        n = tf.shape(K)[0]\n        K += (noise + f64) * tf.eye(n, dtype=tf.float64)\n        \n        L = tf.linalg.cholesky(K)\n        alpha = tf.linalg.cholesky_solve(L, tf.expand_dims(y_tf, 1))\n        \n        log_lik = -half_64 * tf.squeeze(tf.matmul(tf.expand_dims(y_tf, 0), alpha))\n        log_lik -= tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)))\n        n_64 = tf.cast(n, tf.float64)\n        log_lik -= half_64 * n_64 * tf.math.log(two_pi_64)\n        \n        return log_lik\n\n\n    def sample_theta_posterior(self):\n        def target_log_prob_fn(theta):\n            return self.log_posterior(theta, self.X_obs, self.y_obs)\n\n        kernel = tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=target_log_prob_fn,\n            num_leapfrog_steps=3,\n            step_size=0.1)\n        \n        samples = tfp.mcmc.sample_chain(\n            num_results=self.num_samples,\n            current_state=tf.ones(self.theta_dim, dtype=tf.float64),\n            kernel=kernel,\n            num_burnin_steps=self.burn_in,\n            trace_fn=None)\n        \n        return samples.numpy()\n    \n    def expected_improvement(self, x_candidate, theta_samples):\n        if self.y_obs.size == 0: return 1.0\n        \n        y_best = np.max(self.y_obs)\n        x_candidate_reshaped = x_candidate.reshape(1, -1)\n        \n        ei_values = []\n        for theta in theta_samples:\n\n            try:\n                theta_np = theta.numpy() if hasattr(theta, 'numpy') else theta\n                \n                K_obs = self.matern52_kernel(self.X_obs, self.X_obs, theta_np).numpy()\n                noise = theta_np[self.dim + 1]**2\n                K_obs += (noise + 1e-6) * np.eye(len(self.X_obs))\n                \n                K_cross = self.matern52_kernel(self.X_obs, x_candidate_reshaped, theta_np).numpy()\n                K_candidate = self.matern52_kernel(x_candidate_reshaped, x_candidate_reshaped, theta_np).numpy()\n                L = np.linalg.cholesky(K_obs)\n                L_inv_K_cross = np.linalg.solve(L, K_cross)\n                \n                mu = np.dot(L_inv_K_cross.T, np.linalg.solve(L, self.y_obs))\n                sigma2 = K_candidate - np.dot(L_inv_K_cross.T, L_inv_K_cross)\n\n                mu_scalar = mu[0]\n                sigma2_scalar = sigma2[0, 0]\n                \n                sigma = np.sqrt(np.maximum(sigma2_scalar, 1e-9))\n                \n                if sigma == 0.0:\n                    continue\n\n                z = (mu_scalar - y_best - self.exploration) / sigma\n                \n                norm = stats.norm(loc=0, scale=1)\n                ei = (mu_scalar - y_best - self.exploration) * norm.cdf(z) + sigma * norm.pdf(z)\n                \n                if not np.isnan(ei):\n                    ei_values.append(ei)\n\n            except np.linalg.LinAlgError:\n                print(\"警告: 在EI计算中发生矩阵奇异值错误，跳过此theta样本。\")\n                continue\n                \n        return np.mean(ei_values) if ei_values else 0.0\n         \n\n    \n    def _write_log(self, iteration, x_new, y_new):\n        param_names = ['lr','dense_units','dropout_rate','l2_reg','optimizer_choice','momentum']\n        params_dict = {name:val for name, val in zip(param_names, x_new)}\n        log_entry={'iteration':int(iteration),\n                   'accuracy':float(y_new),\n                   'params':params_dict}\n        with open(self.log_file,'a') as f: \n            f.write(json.dumps(log_entry)+'\\n')\n    \n    def optimize(self, objective_fn, n_iter=30):\n        num_existing_points = self.X_obs.shape[0]\n        num_init_points_to_run = max(0, self.init_points - num_existing_points)\n\n        if num_init_points_to_run > 0:\n            print(f\"--- 继续随机初始化阶段: 还需要运行 {num_init_points_to_run} 个初始点。 ---\")\n            for i in range(num_init_points_to_run):\n                print(f\"\\n--- 初始点 {num_existing_points + i + 1}/{self.init_points} ---\")\n                x_new = self.random_sample()\n                y_new = objective_fn(x_new)\n                self.X_obs = np.vstack([self.X_obs, x_new])\n                self.y_obs = np.append(self.y_obs, y_new)\n                self._write_log(iteration=num_existing_points + i + 1, x_new=x_new, y_new=y_new)\n        \n        elif num_existing_points > 0:\n            print(f\"--- 成功恢复优化: 已加载 {num_existing_points} 个点。跳过 {self.init_points} 个初始点的随机搜索阶段。 ---\")\n        \n        else:\n            print(f\"--- 开始全新运行: 执行 {self.init_points} 个初始随机点 ---\")\n            for i in range(self.init_points):\n                print(f\"\\n--- 初始点 {i + 1}/{self.init_points} ---\")\n                x_new = self.random_sample()\n                y_new = objective_fn(x_new)\n                self.X_obs = np.vstack([self.X_obs, x_new])\n                self.y_obs = np.append(self.y_obs, y_new)\n                self._write_log(iteration=i + 1, x_new=x_new, y_new=y_new)\n\n        print(f\"\\n--- 开始执行 {n_iter} 次新的贝叶斯优化迭代 ---\")\n        \n        \n\n        for i in range(n_iter):\n            print(f\"\\n--- 优化迭代 {i+1}/{n_iter} ---\")\n            \n            theta_samples = self.sample_theta_posterior()\n            \n            best_ei = -1\n            best_x = None\n            for _ in range(200): # 增加随机搜索次数以更好地优化采集函数\n                x_candidate = self.random_sample()\n                ei = self.expected_improvement(x_candidate, theta_samples)\n                if ei > best_ei:\n                    best_ei = ei\n                    best_x = x_candidate\n            \n            if best_x is None:\n                print(\"警告：无法找到有效的候选点，将进行随机采样。\")\n                best_x = self.random_sample()\n\n            y_new = objective_fn(best_x)\n            \n            self.X_obs = np.vstack([self.X_obs, best_x])\n            self.y_obs = np.append(self.y_obs, y_new)\n            # --- 在每次优化后也写入日志 ---\n            self._write_log(iteration=self.init_points + i + 1, x_new=best_x, y_new=y_new)\n\n\n        best_idx = np.argmax(self.y_obs)\n        return self.X_obs[best_idx], self.y_obs[best_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport gc \n#---------------------------- main---------------------------------\n\nsearch_space = [\n    (1e-5, 5e-4),    # lr\n    (64, 512),       # dense_units\n    (0.2, 0.7),      # dropout_rate\n    (1e-5, 5e-3),    # l2_reg\n    (0, 3.99),      # 0=Adam, 1=SGD, 2=RMSprop, 3=Adagrad\n    (0.85, 0.99)     # momentum\n]\n\ndef my_objective_fn(x_new):\n    # 将优化器给出的numpy数组转换为超参数字典\n    hparams = {\n        'lr': x_new[0],\n        'dense_units': int(x_new[1]),\n        'dropout_rate': x_new[2],\n        'l2_reg': x_new[3],\n        'optimizer_choice': int(x_new[4]),\n        'momentum': x_new[5],\n    }\n    # 清理会话并调用训练函数\n    tf.keras.backend.clear_session()\n    gc.collect()\n    print(f\"\\n正在评估超参数: {hparams}\")\n    accuracy = train_and_evaluate_model(hparams, train_ds, val_ds, epochs=40)\n    print(f\"评估完成 - 验证集准确率: {accuracy:.4f}\")\n    return accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:52:58.288022Z","iopub.execute_input":"2025-09-08T08:52:58.288533Z","iopub.status.idle":"2025-09-08T08:52:58.293983Z","shell.execute_reply.started":"2025-09-08T08:52:58.288508Z","shell.execute_reply":"2025-09-08T08:52:58.293301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 定义日志文件的路径\nlog_file_path = \"/kaggle/input/ei-mcmc-bo-ver4/EI_MCMC_BO_log12.json\" \nparam_order = ['lr', 'dense_units', 'dropout_rate', 'l2_reg', 'optimizer_choice', 'momentum']\n\nnum_to_load=10 #从日志中加载的数据条数\nnum_new_iter=15 #本次需要接续运行的随机探索次数\n\nX_list = []\ny_list = []\n\nprint(f\"正在从日志文件 '{log_file_path}' 提取前{num_to_load}条数据...\")\ntry:\n    with open(log_file_path, 'r') as f:\n        for i, line in enumerate(f):\n            if i>= num_to_load:\n                break\n            \n            line=line.strip()#去除行首尾的空白字符\n            if not line:\n                continue #强制跳过空行\n            try:\n                log_entry = json.loads(line)#json字符解析为字典\n                y_list.append(log_entry['accuracy'])\n                params_dict = log_entry['params']\n                param_vector=[params_dict[key] for key in param_order]\n                X_list.append(param_vector)\n            except (json.JSONDecodeError, KeyError) as e:\n                print(f\"跳过格式错误或缺少键值的行: {line} | 错误: {e}\")\n    # 4. 将Python列表转换为Numpy数组\n    initial_X = np.array(X_list)\n    initial_y = np.array(y_list)\n    print(f\"✅ 成功加载了 {len(initial_y)} 条历史数据。\")\n    print(f\"   - initial_X 的形状: {initial_X.shape}\")\n    print(f\"   - initial_y 的形状: {initial_y.shape}\")\n    \n\nexcept FileNotFoundError:\n    print(f\"❌ 错误: 日志文件未找到，请检查路径: '{log_file_path}'\")\n    initial_X, initial_y = None, None\n\n\n\n\n\nif initial_X is not None:\n    num_init=num_to_load+num_new_iter\n\n    num_loaded = len(initial_y)\n    num_remaining_explore = max(0, num_init - num_loaded)\n    print(\"-\" * 30)\n    print(f\"恢复运行计算:\")\n    print(f\"  已成功加载历史数据点: {num_loaded}\")\n    print(f\"  目标最终总点数: {num_init}\")\n    print(f\"  剩余探索点数: {num_remaining_explore}\")\n    print(\"-\" * 30)\n\n    optimizer = BayesianOptimizer(\n        search_space=search_space,\n        init_points=num_init,\n        log_file=\"resumed_random_search.json\", \n        initial_X=initial_X,\n        initial_y=initial_y\n    )\n\n    best_x, best_y = optimizer.optimize(\n        objective_fn=my_objective_fn, \n        n_iter=0 \n    )\n    print(\"\\n\" + \"=\"*50)\n    print(\"所有随机探索已完成!\")\n    print(f\"在总共 {num_init} 次随机探索中，最佳验证准确率: {best_y:.4f}\")\n\n    best_hparams = {name: val for name, val in zip(param_order, best_x)}\n    best_hparams['dense_units'] = int(best_hparams['dense_units'])\n    best_hparams['optimizer_choice'] = int(best_hparams['optimizer_choice'])\n\n    print(f\"对应的最佳超参数: {best_hparams}\")\n    print(\"=\"*50)\n\nelse:\n    print(\"\\n未能加载初始数据，无法继续运行。\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:06:16.627636Z","iopub.execute_input":"2025-09-16T05:06:16.627903Z","iopub.status.idle":"2025-09-16T05:06:16.646448Z","shell.execute_reply.started":"2025-09-16T05:06:16.627884Z","shell.execute_reply":"2025-09-16T05:06:16.64551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 随机搜索全过程的图像表示 \n# ===================================================================\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 设置全局绘图风格\nsns.set_style('whitegrid')\n\n\nlog_file_path = \"resumed_random_search.json\" \n\ntry:\n    with open(log_file_path, 'r') as f:\n        logs = [json.loads(line) for line in f]\n    \n    df_results = pd.DataFrame(logs)\n    all_accuracies = df_results['accuracy'].values\n    print(f\"✅ 日志文件 '{log_file_path}' 加载成功! 共加载 {len(all_accuracies)} 条评估记录。\")\n    \n    \n    if len(all_accuracies) > 0:\n        # 找到全局最优的准确率\n        best_final_accuracy = np.max(all_accuracies)\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"随机搜索结果分析:\")\n        print(\"=\"*50)\n        print(f\"- 执行总次数: {len(all_accuracies)} 次\")\n        print(f\"- 全局最优验证集准确率 (Val Accuracy): {best_final_accuracy:.4f}\")\n        print(\"=\"*50)\n    else:\n        print(\"\\n日志文件中没有评估记录。\")\n\n    # --- 3. 创建图表---\n\n    # 创建从1开始的x轴\n    n_evaluations = len(all_accuracies)\n    x_axis = np.arange(1, n_evaluations + 1)\n\n    # 找到最佳点的坐标\n    best_idx = np.argmax(all_accuracies)\n    best_x = best_idx + 1 # 评估次数 (从1开始)\n    best_y = all_accuracies[best_idx]\n\n    # 开始绘图\n    plt.figure(figsize=(14, 8))\n    \n    # 绘制每次评估准确率的散点/折线图\n    plt.plot(x_axis, all_accuracies, \n             'o-', label='Evaluation Accuracy of a single assessment',\n             color='forestgreen', alpha=0.7)\n\n    # 绘制迄今为止最优准确率的曲线\n    plt.plot(x_axis, np.maximum.accumulate(all_accuracies), \n             'r--', label='Progressive Best', linewidth=2.5)\n\n    # 用一个金色星星突出显示最佳点\n    plt.plot(best_x, best_y, \n             marker='*', \n             color='green', \n             markersize=25, \n             markeredgecolor='black',\n             linestyle='', # 无连接线\n             label=f'global best point (Val acc: {best_y:.4f})\\n in the {best_x}th assessment')\n\n\n    plt.xlabel('Number of Evaluations', fontsize=12)\n    plt.ylabel('Validation Accuracy', fontsize=12)\n    plt.title(f'Random search progress curve (global best val acc: {best_final_accuracy:.4f})', fontsize=16) # ★ 修改了标题\n    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n    plt.legend(fontsize=12)\n    plt.tight_layout()\n    plt.savefig('random_search_curve_35_points.png')\n    plt.show()\n\nexcept FileNotFoundError:\n    print(f\"错误: 日志文件 '{log_file_path}' 未找到。\")\nexcept Exception as e:\n    print(f\"处理或绘图时发生错误: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T04:19:48.672687Z","iopub.execute_input":"2025-09-16T04:19:48.672983Z","iopub.status.idle":"2025-09-16T04:19:49.494742Z","shell.execute_reply.started":"2025-09-16T04:19:48.67296Z","shell.execute_reply":"2025-09-16T04:19:49.494033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}