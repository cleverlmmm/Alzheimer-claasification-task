{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13155774,"sourceType":"datasetVersion","datasetId":8335565}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"===============\n画图：积分型取平均的EI采集函数取点的演示图\n===============\n具体的实现是，导入30次BO数据点的json文件，提取最优的那一组超参数配置，固定除了lr之外的其他超参（保持与最优配置里的一样），相当于把6维的超参映射到lr这一维度，画出lr相关的采集函数图","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport scipy.stats as stats\nimport os\nimport random\nimport time\nimport gc\n\n\ntfd = tfp.distributions\ntf.keras.backend.set_floatx('float64') # 确保数据类型一致\n\nclass BayesianOptimizer:\n    def __init__(self, search_space, init_points=5, exploration=0.01, \n                 log_file=\"custom_bo_log.json\", initial_X=None, initial_y=None):\n        self.search_space = np.array(search_space)\n        self.dim = len(search_space)\n        self.init_points = init_points\n        self.exploration = exploration\n        self.log_file = log_file\n        #----以防训练由于超时中断,设置自断点处接着BO迭代,加载已计算好的数据\n        if initial_X is not None and initial_y is not None:\n            self.X_obs = initial_X\n            self.y_obs = initial_y\n            print(f\"BO optimizer has used {len(self.y_obs)} observed datas to initialize\")\n            #日志回填\n            print(f\"new log '{self.log_file}' is set and loads old data into it \")\n            if os.path.exists(self.log_file):\n                os.remove(self.log_file)\n            for i in range(len(self.y_obs)):\n                self._write_log(iteration=i + 1, x_new=self.X_obs[i], y_new=self.y_obs[i])\n            print(f\"历史数据回填完成。共写入 {len(self.y_obs)} 条记录。\")\n        else: \n            self.X_obs = np.zeros((0, self.dim))\n            self.y_obs = np.zeros(0)\n            if os.path.exists(self.log_file):\n                os.remove(self.log_file)\n                \n        self.theta_dim = self.dim + 2 # length_scales (dim) + amplitude + noise\n        \n        self.num_samples = 20\n        self.burn_in = 100\n        \n    \n        \n    def random_sample(self):\n        \"\"\"在搜索空间内随机采样一个超参数点。\"\"\"\n        return np.array([np.random.uniform(low, high) for (low, high) in self.search_space])\n\n    @tf.function(jit_compile=True) # 使用XLA编译加速\n    def matern52_kernel(self, X1, X2, theta):\n        X1_tf = tf.cast(X1, tf.float64)\n        X2_tf = tf.cast(X2, tf.float64)\n        theta = tf.cast(theta, tf.float64)\n        \n        length_scales = theta[:self.dim]\n        amplitude = theta[self.dim]\n        \n        X1_scaled = X1_tf / length_scales\n        X2_scaled = X2_tf / length_scales\n\n        r2 = tf.reduce_sum(tf.square(X1_scaled), axis=1, keepdims=True) - 2 * tf.matmul(X1_scaled, X2_scaled, transpose_b=True) + tf.reduce_sum(tf.square(X2_scaled), axis=1)\n        r2 = tf.maximum(r2, 1e-12)\n        r = tf.sqrt(r2)\n        \n        five_64 = tf.constant(5.0, dtype=tf.float64)\n        three_64 = tf.constant(3.0, dtype=tf.float64)\n        one_64 = tf.constant(1.0, dtype=tf.float64)\n        \n        sqrt5_64 = tf.sqrt(five_64)\n        \n        term = (one_64 + sqrt5_64 * r + (five_64 / three_64) * r2) * tf.exp(-sqrt5_64 * r)\n        \n        K = amplitude**2 * term\n        return K\n\n    \n    @tf.function(jit_compile=True) # 使用XLA编译加速\n    def log_posterior(self, theta, X, y):\n        f64 = tf.constant(1e-6, dtype=tf.float64)\n        two_pi_64 = tf.constant(2.0 * np.pi, dtype=tf.float64)\n        half_64 = tf.constant(0.5, dtype=tf.float64)\n        \n        X_tf = tf.cast(X, dtype=tf.float64)\n        y_tf = tf.cast(y, dtype=tf.float64)\n        theta_tf = tf.cast(theta, dtype=tf.float64)\n        \n        noise = theta_tf[self.dim + 1]**2\n        \n        K = self.matern52_kernel(X_tf, X_tf, theta_tf)\n        n = tf.shape(K)[0]\n        K += (noise + f64) * tf.eye(n, dtype=tf.float64)\n        \n        L = tf.linalg.cholesky(K)\n        alpha = tf.linalg.cholesky_solve(L, tf.expand_dims(y_tf, 1))\n        \n        log_lik = -half_64 * tf.squeeze(tf.matmul(tf.expand_dims(y_tf, 0), alpha))\n        log_lik -= tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)))\n        n_64 = tf.cast(n, tf.float64)\n        log_lik -= half_64 * n_64 * tf.math.log(two_pi_64)\n        \n        return log_lik\n\n\n    def sample_theta_posterior(self):\n        def target_log_prob_fn(theta):\n            return self.log_posterior(theta, self.X_obs, self.y_obs)\n\n        kernel = tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=target_log_prob_fn,\n            num_leapfrog_steps=3,\n            step_size=0.1)\n        \n        samples = tfp.mcmc.sample_chain(\n            num_results=self.num_samples,\n            current_state=tf.ones(self.theta_dim, dtype=tf.float64),\n            kernel=kernel,\n            num_burnin_steps=self.burn_in,\n            trace_fn=None)\n        \n        return samples.numpy()\n    \n    def expected_improvement(self, x_candidate, theta_samples):\n        if self.y_obs.size == 0: return 1.0\n        \n        y_best = np.max(self.y_obs)\n        x_candidate_reshaped = x_candidate.reshape(1, -1)\n        \n        ei_values = []\n        for theta in theta_samples:\n\n            try:\n                theta_np = theta.numpy() if hasattr(theta, 'numpy') else theta\n                \n                K_obs = self.matern52_kernel(self.X_obs, self.X_obs, theta_np).numpy()\n                noise = theta_np[self.dim + 1]**2\n                K_obs += (noise + 1e-6) * np.eye(len(self.X_obs))\n                \n                K_cross = self.matern52_kernel(self.X_obs, x_candidate_reshaped, theta_np).numpy()\n                K_candidate = self.matern52_kernel(x_candidate_reshaped, x_candidate_reshaped, theta_np).numpy()\n                L = np.linalg.cholesky(K_obs)\n                L_inv_K_cross = np.linalg.solve(L, K_cross)\n                \n                mu = np.dot(L_inv_K_cross.T, np.linalg.solve(L, self.y_obs))\n                sigma2 = K_candidate - np.dot(L_inv_K_cross.T, L_inv_K_cross)\n\n                mu_scalar = mu[0]\n                sigma2_scalar = sigma2[0, 0]\n                \n                sigma = np.sqrt(np.maximum(sigma2_scalar, 1e-9))\n                if sigma == 0.0:\n                    continue\n\n                z = (mu_scalar - y_best - self.exploration) / sigma\n                \n                norm = stats.norm(loc=0, scale=1)\n                ei = (mu_scalar - y_best - self.exploration) * norm.cdf(z) + sigma * norm.pdf(z)\n                \n                if not np.isnan(ei):\n                    ei_values.append(ei)\n\n            except np.linalg.LinAlgError:\n                print(\"警告: 在EI计算中发生矩阵奇异值错误，跳过此theta样本。\")\n                continue\n                \n        return np.mean(ei_values) if ei_values else 0.0\n         \n\n    \n    def _write_log(self, iteration, x_new, y_new):\n        param_names = ['lr','dense_units','dropout_rate','l2_reg','optimizer_choice','momentum']\n        params_dict = {name:val for name, val in zip(param_names, x_new)}\n        log_entry={'iteration':int(iteration),\n                   'accuracy':float(y_new),\n                   'params':params_dict}\n        with open(self.log_file,'a') as f: \n            f.write(json.dumps(log_entry)+'\\n')\n    \n    def optimize(self, objective_fn, n_iter=30):\n        #检查是 初次BO 还是 接续BO\n        num_existing_points=self.X_obs.shape[0]\n        num_init_points_to_run = max(0, self.init_points - num_existing_points)\n\n        if num_init_points_to_run > 0:\n            # 假如加载的点数少于要求的初始随机化点数，需要补充运行\n            print(f\"--- 继续随机初始化阶段: 还需要运行 {num_init_points_to_run} 个初始点。 ---\")\n            for i in range(num_init_points_to_run):\n                print(f\"\\n--- 初始点 {num_existing_points + i + 1}/{self.init_points} ---\")\n                x_new = self.random_sample()\n                y_new = objective_fn(x_new)\n                self.X_obs = np.vstack([self.X_obs, x_new])\n                self.y_obs = np.append(self.y_obs, y_new)\n                self._write_log(iteration=num_existing_points + i + 1, x_new=x_new, y_new=y_new)\n        \n        elif num_existing_points > 0:\n            # 成功恢复运行。加载的点数已满足要求，跳过随机搜索。\n            print(f\"--- 成功恢复优化: 已加载 {num_existing_points} 个点。跳过 {self.init_points} 个初始点的随机搜索阶段。 ---\")\n        \n        else:\n            #全新运行 (没有加载任何数据)\n            print(f\"--- 开始全新运行: 执行 {self.init_points} 个初始随机点 ---\")\n            for i in range(self.init_points):\n                print(f\"\\n--- 初始点 {i + 1}/{self.init_points} ---\")\n                x_new = self.random_sample()\n                y_new = objective_fn(x_new)\n                self.X_obs = np.vstack([self.X_obs, x_new])\n                self.y_obs = np.append(self.y_obs, y_new)\n                self._write_log(iteration=i + 1, x_new=x_new, y_new=y_new)\n\n        print(f\"\\n--- 开始执行 {n_iter} 次新的贝叶斯优化迭代 ---\")\n\n        \n        for i in range(n_iter):\n            print(f\"\\n--- 优化迭代 {i+1}/{n_iter} ---\")\n            \n            theta_samples = self.sample_theta_posterior()\n            \n            best_ei = -1\n            best_x = None\n            for _ in range(200): # 增加随机搜索次数以更好地优化采集函数\n                x_candidate = self.random_sample()\n                ei = self.expected_improvement(x_candidate, theta_samples)\n                if ei > best_ei:\n                    best_ei = ei\n                    best_x = x_candidate\n            \n            if best_x is None:\n                print(\"警告：无法找到有效的候选点，将进行随机采样。\")\n                best_x = self.random_sample()\n\n            y_new = objective_fn(best_x)\n            \n            self.X_obs = np.vstack([self.X_obs, best_x])\n            self.y_obs = np.append(self.y_obs, y_new)\n            # 在每次优化后也写入日志 ---\n            new_iteration_number = self.X_obs.shape[0]\n            self._write_log(iteration=new_iteration_number, x_new=best_x, y_new=y_new)\n\n\n        best_idx = np.argmax(self.y_obs)\n        return self.X_obs[best_idx], self.y_obs[best_idx]\n\n\n    def get_slice_visualization_data(self, x_anchor, var_param_index, num_grid_points=100):\n        #x_anchor: 6维“锚点”，即BO找到的当前最佳点\n        #var_param_index:切片的投影维度\n        #num_grid_points:横坐标网格数量,用来绘制平滑曲线\n        print(\"正在使用HMC采样GP的超参数 (theta)...\")\n        # 1. 获取 theta 样本\n        try:\n            theta_samples = self.sample_theta_posterior() \n        except tf.errors.InvalidArgumentError as e:\n            print(f\"错误：HMC采样失败。{e}\")\n            return None\n            \n        # 2. 定义 1D 切片 (X-axis)\n        var_space = self.search_space[var_param_index]\n        is_log_scale = (var_space[1] / var_space[0] > 100) \n        if is_log_scale:\n            x_axis = np.logspace(np.log10(var_space[0]), np.log10(var_space[1]), num_grid_points)\n        else:\n            x_axis = np.linspace(var_space[0], var_space[1], num_grid_points)\n\n        # 3. 创建 6D 测试网格\n        X_grid = np.tile(x_anchor, (num_grid_points, 1))\n        #np.tile就是行复制\n        X_grid[:, var_param_index] = x_axis\n\n        # 4. 循环遍历每个 theta 样本\n        y_best = np.max(self.y_obs)\n        all_mu_curves = []\n        all_sigma_curves = []\n        all_ei_curves = []\n        \n        print(f\"正在为 {len(theta_samples)} 个 theta 样本计算后验和EI曲线...\")\n        for theta in theta_samples:\n            try:\n                # ... (GP回归和EI计算) ...\n                theta_np = theta.numpy() if hasattr(theta, 'numpy') else theta\n                noise = theta_np[self.dim + 1]**2 + 1e-6\n                K_obs = self.matern52_kernel(self.X_obs, self.X_obs, theta_np).numpy()\n                K_obs += np.eye(len(self.X_obs)) * noise\n                K_cross = self.matern52_kernel(self.X_obs, X_grid, theta_np).numpy()\n                K_grid = self.matern52_kernel(X_grid, X_grid, theta_np).numpy()\n                L = np.linalg.cholesky(K_obs)\n                alpha_ = np.linalg.solve(L, self.y_obs)\n                alpha = np.linalg.solve(L.T, alpha_)\n                mu_grid = K_cross.T @ alpha\n                v = np.linalg.solve(L, K_cross) \n                var_grid = np.diag(K_grid) - np.sum(v**2, axis=0)\n                sigma_grid = np.sqrt(np.maximum(var_grid, 1e-9))\n                z = (mu_grid - y_best - self.exploration) / sigma_grid\n                norm_dist = stats.norm(loc=0, scale=1)\n                ei_grid = (mu_grid - y_best - self.exploration) * norm_dist.cdf(z) + sigma_grid * norm_dist.pdf(z)\n                ei_grid[sigma_grid == 0.0] = 0.0\n                all_mu_curves.append(mu_grid)\n                all_sigma_curves.append(sigma_grid)\n                all_ei_curves.append(ei_grid)\n            except np.linalg.LinAlgError:\n                continue \n\n        # 5. 计算积分EI\n        integrated_ei = np.mean(all_ei_curves, axis=0)\n\n        print(\"✅ 切片数据计算完成。\")\n        return x_axis, all_mu_curves, all_sigma_curves, all_ei_curves, integrated_ei, is_log_scale\n\n   \n            \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:20:56.842794Z","iopub.execute_input":"2025-11-05T07:20:56.843115Z","iopub.status.idle":"2025-11-05T07:21:02.760753Z","shell.execute_reply.started":"2025-11-05T07:20:56.843088Z","shell.execute_reply":"2025-11-05T07:21:02.759861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n#加载数据并实例化优化器\n# ===================================================================\n\nsearch_space = [\n    (1e-5, 5e-4),    # lr\n    (64, 512),       # dense_units\n    (0.2, 0.7),      # dropout_rate\n    (1e-5, 5e-3),    # l2_reg\n    (0, 3.99),      # 0=Adam, 1=SGD, 2=RMSprop, 3=Adagrad\n    (0.85, 0.99)     # momentum\n]\n\n\n\n#加载日志文件-继续上次运行\nlog_file_path = \"/kaggle/input/bo-data-30/second_continued_EI_MCMC_BO_log30.json\" \n\nparam_order=['lr', 'dense_units', 'dropout_rate', 'l2_reg', 'optimizer_choice', 'momentum']\nX_list_full = []\ny_list_full = []\nnum_points_to_load = 10 \nprint(f\"--- 正在从 '{log_file_path}' 提取前 {num_points_to_load} 条数据 ---\")\ntry:\n    with open(log_file_path, 'r') as f:\n        for i, line in enumerate(f):\n            if i >= num_points_to_load: \n                break\n            \n            line = line.strip()\n            if not line: continue\n            \n            log_entry = json.loads(line)\n            y_list_full.append(log_entry['accuracy'])\n            params_dict = log_entry['params']\n            X_list_full.append([params_dict[key] for key in param_order])\n\n    initial_X_early = np.array(X_list_full)\n    initial_y_early = np.array(y_list_full)\n\n    if len(initial_y_early) < num_points_to_load:\n        print(f\"❌ 警告: 只加载了 {len(initial_y_early)} 个点，少于目标的 {num_points_to_load} 个。\")\n\n    print(f\"成功加载了 {len(initial_y_early)} 条历史数据。\")\n\n    optimizer_early_stage = BayesianOptimizer(\n        search_space=search_space,\n        log_file=\"visualization_log_early.json\", \n        initial_X=initial_X_early,\n        initial_y=initial_y_early\n    )\n    print(\" 早期阶段优化器实例已创建，并加载了 5 个数据点。\")\n    print(f\"   - 内部 X_obs 形状: {optimizer_early_stage.X_obs.shape}\")\n    print(f\"   - 内部 y_obs 形状: {optimizer_early_stage.y_obs.shape}\")\n\nexcept FileNotFoundError:\n    print(f\"❌ 错误: 日志文件未找到，请检查路径: '{log_file_path}'\")\n    optimizer_early_stage = None\nexcept Exception as e:\n    print(f\"❌ 加载数据时出错: {e}\")\n    optimizer_early_stage = None\n    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:21:02.762008Z","iopub.execute_input":"2025-11-05T07:21:02.762627Z","iopub.status.idle":"2025-11-05T07:21:02.79006Z","shell.execute_reply.started":"2025-11-05T07:21:02.762595Z","shell.execute_reply":"2025-11-05T07:21:02.789333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 运行可视化绘图 (标签版)\n# ===================================================================\n\nfrom matplotlib import rcParams \nrcParams['font.weight'] = 'bold'\nrcParams['axes.labelweight'] = 'bold'\n\n# 检查“早期阶段”优化器对象是否存在 ---\nif 'optimizer_early_stage' in locals() and optimizer_early_stage.X_obs.shape[0] > 0:\n\n    current_best_idx = np.argmax(optimizer_early_stage.y_obs)\n    x_anchor = optimizer_early_stage.X_obs[current_best_idx]\n    var_param_index = 1 \n    var_param_name = 'dense_units' \n    vis_data = optimizer_early_stage.get_slice_visualization_data(\n        x_anchor=x_anchor,\n        var_param_index=var_param_index,\n        num_grid_points=200 \n    )\nelse:\n    print(\"❌ 错误: 'optimizer_early_stage' 对象未准备好。请先运行上一个单元格。\")\n    vis_data = None\n\n# 开始绘图 (3个子图) ---\nif vis_data:\n    x_axis, all_mu, all_sigma, all_ei, integrated_ei_all, is_log_scale = vis_data\n    \n    print(\"正在绘制图表...\")\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 7), sharex=True)\n    \n    latex_labels = [r'Posterior Mean $\\mu(\\mathbf{h} ; \\psi^{(1)})$', \n                    r'Posterior Mean $\\mu(\\mathbf{h} ; \\psi^{(2)})$', \n                    r'Posterior Mean $\\mu(\\mathbf{h} ; \\psi^{(3)})$']\n    ei_labels = [r'$a_{\\mathrm{EI}}(\\mathbf{h} ; \\psi^{(1)}$)', \n                 r'$a_{\\mathrm{EI}}(\\mathbf{h} ; \\psi^{(2)}$)', \n                 r'$a_{\\mathrm{EI}}(\\mathbf{h} ; \\psi^{(3)}$)']\n    \n    legend_props = {'loc': 'lower right', 'prop': {'weight': 'bold', 'size': 14}}\n\n    # --- (a) 绘制后验样本 (Posterior Samples) ---\n    colors = ['C3', 'C2', 'C0'] # (红, 绿, 蓝)\n    for i in range(min(3, len(all_mu))):\n        mu, sigma = all_mu[i], all_sigma[i]\n        axes[0].plot(x_axis, mu, color=colors[i], lw=2, label=latex_labels[i])\n        axes[0].fill_between(x_axis, mu - 1.96 * sigma, mu + 1.96 * sigma, \n                             color=colors[i], alpha=0.15) # 重新启用置信区间\n    \n    # 将标题放回顶部\n    axes[0].set_title('(a) Posterior Predictive Means', fontsize=20, fontweight='bold')\n    axes[0].set_xlabel('') # 顶部图表不需要 X 标签\n    axes[0].set_ylabel('Expected Validation Accuracy', fontsize=18, fontweight='bold')\n    axes[0].legend(**legend_props) \n\n    # --- (b) 绘制对应的 EI 曲线 ---\n    ei_curves_to_plot = [] \n    for i in range(min(3, len(all_ei))):\n        ei = all_ei[i]\n        ei_curves_to_plot.append(ei) \n        axes[1].plot(x_axis, ei, color=colors[i], lw=2, label=ei_labels[i])\n        max_ei_idx = np.argmax(ei)\n        axes[1].plot(x_axis[max_ei_idx], ei[max_ei_idx], marker='o', \n                     color=colors[i], markersize=12, markeredgecolor='black')\n    \n    axes[1].set_title('(b) Expected Improvement Functions', fontsize=20, fontweight='bold')\n    axes[1].set_xlabel('') # 中间图表不需要 X 标签\n    axes[1].set_ylabel('Expected Improvement (EI)', fontsize=18, fontweight='bold')\n    axes[1].legend(**legend_props) \n\n    # --- (c) 绘制 【前三条EI曲线】 的平均值 ---\n    if ei_curves_to_plot:\n        integrated_ei_3 = np.mean(ei_curves_to_plot, axis=0) \n        \n        axes[2].plot(x_axis, integrated_ei_3, color='black', lw=2.5, \n                     label=r'Integrated EI $a_{\\mathrm{BO}}(\\mathbf{h})$')\n        \n        max_iei_idx = np.argmax(integrated_ei_3)\n        axes[2].plot(x_axis[max_iei_idx], integrated_ei_3[max_iei_idx], marker='o', \n                     color='black', markersize=12, label='Chosen Next Point')\n    \n    axes[2].set_title('(c) The Integrated Acquisition Function', fontsize=20, fontweight='bold')\n    axes[2].set_ylabel('Integrated EI', fontsize=18, fontweight='bold')\n    axes[2].legend(**legend_props) \n    \n    if is_log_scale: axes[2].set_xscale('log') \n    \n    for ax in axes:\n        ax.grid(True, linestyle='--', alpha=0.6)\n        if is_log_scale: ax.set_xscale('log')\n        for label in ax.get_xticklabels() + ax.get_yticklabels():\n            label.set_fontweight('bold')\n            \n    plt.tight_layout() \n    \n    plt.savefig('full_bo_visualization_slice_early_stage.pdf', format='pdf', bbox_inches='tight')\n    plt.show()\n\nplt.rcdefaults()\nprint(\"\\n[INFO] Matplotlib 字体设置已重置为默认值。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:26:43.039278Z","iopub.execute_input":"2025-11-05T07:26:43.039645Z","iopub.status.idle":"2025-11-05T07:26:48.605938Z","shell.execute_reply.started":"2025-11-05T07:26:43.039623Z","shell.execute_reply":"2025-11-05T07:26:48.60512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}