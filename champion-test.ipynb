{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12107936,"sourceType":"datasetVersion","datasetId":7623043},{"sourceId":13155774,"sourceType":"datasetVersion","datasetId":8335565},{"sourceId":13155779,"sourceType":"datasetVersion","datasetId":8335569}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"ä»…è¿è¡Œchampion cellå—ï¼Œéœ€è¦å¯¼å…¥BOè¿­ä»£å’ŒRSè¿­ä»£çš„æ—¥å¿—ç»“æœ \n","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\n\n# å¯¼å…¥ç³»ç»Ÿåº“\n\nimport time\nimport shutil\nimport pathlib\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import keras\nfrom keras import models, optimizers, metrics, layers, regularizers, losses\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad\n\nimport json\n\ndef set_reproducibility(seed=88):\n    \"\"\"\n    è®¾ç½®ä¸€ä¸ªå…¨å±€ç§å­æ¥ç¡®ä¿æ•´ä¸ªå®éªŒæµç¨‹çš„å¯å¤ç°æ€§ã€‚\n    è¿™åŒ…æ‹¬Pythonå†…ç½®randomã€osã€NumPyå’ŒTensorFlowã€‚\n    åŒæ—¶ï¼Œå®ƒä¼šå°è¯•é…ç½®TensorFlowä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ã€‚\n    \n    Args:\n        seed (int): æ‚¨é€‰æ‹©çš„éšæœºç§å­ã€‚\n    \"\"\"\n    random.seed(seed)\n    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.config.experimental.enable_op_determinism()\n    \n    print(f\"âœ… å…¨å±€éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}ï¼Œå¹¶å·²å¯ç”¨TensorFlowç¡®å®šæ€§æ“ä½œã€‚\")\n\n\nSEED_VALUE = 88 \nset_reproducibility(SEED_VALUE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T12:57:37.118465Z","iopub.execute_input":"2025-09-15T12:57:37.118743Z","iopub.status.idle":"2025-09-15T12:57:57.655491Z","shell.execute_reply.started":"2025-09-15T12:57:37.118719Z","shell.execute_reply":"2025-09-15T12:57:57.654554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#æ•°æ®é›†ä¼šè‡ªåŠ¨è§£å‹ç¼©ï¼Œåªéœ€è¦ç›´æ¥è®²æ•°æ®åŠ è½½åˆ°outputä¸­\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nimport matplotlib.image as img\n\n\n'''åŠ è½½åˆ°outputè·¯å¾„å¹¶åˆ’åˆ†æ–‡ä»¶å¤¹'''\n\n!pip install split-folders -q\nimport splitfolders\ninput_dir=\"/kaggle/input/augmented-alzheimer-data/augmented_data\"\noutput_dir=\"./output\"\nsplitfolders.ratio(\n    input_dir, \n    output=output_dir, \n    seed=1345, \n    ratio=(.70, 0.15,0.15),\n    group_prefix=None \n) \nprint(\"Dataset split completed!\")\n\n#éªŒè¯è¾“å‡ºè·¯å¾„æ–‡ä»¶å¤¹æ–‡ä»¶æ•°é‡\ndef count_files(directory):\n    for split in ['train', 'val', 'test']:\n        split_path = os.path.join(directory, split)\n        print(f\"\\n{split.upper()} set:\")\n        for class_name in os.listdir(split_path):\n            class_path = os.path.join(split_path, class_name)\n            num_files = len(os.listdir(class_path))\n            print(f\"  {class_name}: {num_files} images\")\n\ncount_files(\"./output\")\n\nAUTOTUNE = tf.data.AUTOTUNE\n'''DATALOAD\nä»åˆ’åˆ†å¥½çš„æ•°æ®é›†å­ç›®å½•train..åŠ è½½æ•°æ®ï¼Œè½¬æ¢ä¸º TensorFlow çš„ Dataset å¯¹è±¡ï¼Œä¾¿äºåç»­è®­ç»ƒ'''\nIMG_HEIGHT = 224\nIMG_WIDTH = 224 \nbatch_size=32\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/train\",\nseed=123,\nlabel_mode='int',\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=True\n).prefetch(buffer_size=AUTOTUNE)\n\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/test\",\nlabel_mode='int',\nseed=123,\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=False  \n).prefetch(buffer_size=AUTOTUNE)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/val\",\nlabel_mode='int',\nseed=123,\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=False\n).prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:08:39.170445Z","iopub.execute_input":"2025-09-15T13:08:39.1712Z","iopub.status.idle":"2025-09-15T13:10:30.372566Z","shell.execute_reply.started":"2025-09-15T13:08:39.17116Z","shell.execute_reply":"2025-09-15T13:10:30.371824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, time, gc, json\nimport numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nimport tensorflow as tf, tensorflow_probability as tfp\nfrom tensorflow import keras\nfrom keras import layers, regularizers\nimport scipy.stats as stats\ntfd = tfp.distributions\nsns.set_style('whitegrid')\ntf.keras.backend.set_floatx('float64')\n\n\n#-----------------------------æ¨¡å‹æ¶æ„æ­å»ºï¼šé¡ºåºæ¨¡å‹------------------------------\ndef build_pretrained_model(hparams):\n    # 1. åŠ è½½é¢„è®­ç»ƒçš„EfficientNetï¼Œä¸åŒ…å«é¡¶éƒ¨çš„åˆ†ç±»å±‚\n    base_model = tf.keras.applications.EfficientNetB0(\n        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    # 2. å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡\n    base_model.trainable = False\n    \n    # 3. ç¡®å®šä»å“ªä¸ªå—å¼€å§‹è§£å†»ï¼ˆå¾®è°ƒï¼‰\n    fine_tune_at_block = 'block5a_expand_conv'\n    # 3. éå†æ‰€æœ‰å±‚ï¼Œè§£å†» fine_tune_at_block åŠä¹‹åçš„æ‰€æœ‰å±‚\n    set_trainable = False\n    for layer in base_model.layers:\n        if layer.name == fine_tune_at_block:\n            set_trainable = True\n        if set_trainable:\n            layer.trainable = True\n        else:\n            layer.trainable = False\n\n    # æœ€ä½³å®è·µï¼šæ— è®ºæ˜¯å¦è§£å†»ï¼Œéƒ½å»ºè®®ä¿æŒ BatchNormalization å±‚å¤„äºå†»ç»“çŠ¶æ€\n    # åœ¨å¾®è°ƒæ—¶ï¼Œç”¨å°æ‰¹é‡æ•°æ®æ›´æ–°BNå±‚çš„ç»Ÿè®¡æ•°æ®å¯èƒ½ä¼šå¼•å…¥å™ªå£°ï¼Œç ´åé¢„è®­ç»ƒå­¦åˆ°çš„åˆ†å¸ƒ\n    for layer in base_model.layers:\n        if isinstance(layer, layers.BatchNormalization):\n            layer.trainable = False\n    \n    dense_units = int(hparams.get('dense_units', 128))\n    dropout_rate = hparams.get('dropout_rate', 0.5)\n    l2_reg = hparams.get('l2_reg', 0.001)\n    lr = hparams.get('lr', 0.0001)\n    optimizer_choice = int(hparams.get('optimizer_choice', 0))\n    momentum = hparams.get('momentum', 0.9)\n    \n    \n    # 3. åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸Šï¼Œæ„å»ºä¸€ä¸ªã€å°è€Œç²¾ã€‘çš„åˆ†ç±»å¤´\n    model = tf.keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(), # ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–æ›¿ä»£Flattenï¼Œå‚æ•°æ›´å°‘ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆ\n        layers.Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)), # ä½¿ç”¨æ›´å°çš„L2å€¼\n        layers.Dropout(dropout_rate),\n        layers.Dense(4, activation='softmax') # 4ä¸ªç±»åˆ«\n    ])\n\n    # optimizer = tf.keras.optimizers.SGD(\n    #     learning_rate=lr,\n    #     momentum=momentum,\n    #     nesterov=False # æš‚æ—¶ä¸ä½¿ç”¨NesterovåŠ¨é‡\n    #     )\n    if optimizer_choice == 0:\n        print(\"--- ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ ---\")\n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    elif optimizer_choice == 1:\n        print(f\"--- ä½¿ç”¨ SGD ä¼˜åŒ–å™¨ (momentum={momentum:.3f}) ---\")\n        optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n    elif optimizer_choice == 2:\n        print(\"--- ä½¿ç”¨ RMSprop ä¼˜åŒ–å™¨ ---\")\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n    else: # optimizer_choice == 3\n        print(\"--- ä½¿ç”¨ Adagrad ä¼˜åŒ–å™¨ ---\")\n        optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n\n    \n    model.compile(\n        optimizer=optimizer,\n        #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        #from_logits=True \n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n   \n    return model\n\n\n\n#---------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------\nclass TrainingCallback(keras.callbacks.Callback):\n    def __init__(self, epochs):\n        super().__init__()\n        self.epochs = epochs\n    def on_train_begin(self, logs=None):\n        header = \"{0:^8s}{1:^11s}{2:^11s}{3:^12s}{4:^12s}{5:^12s}\".format('Epoch', 'Loss', 'Acc', 'Val_Loss', 'Val_Acc', 'Duration')\n        print(header)\n    def on_epoch_begin(self, epoch, logs=None): \n        self.ep_start = time.time()\n    def on_epoch_end(self, epoch, logs=None):\n        duration = time.time() - self.ep_start\n        acc = logs.get('accuracy', 0); val_acc = logs.get('val_accuracy', 0)\n        loss = logs.get('loss', 0); val_loss = logs.get('val_loss', 0)\n        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^10.3f}{acc * 100:^11.3f}{val_loss:^12.5f}{val_acc * 100:^12.3f}{duration:^12.2f}'\n        print(msg)\n        \n\n\n#--------------------------------------------------------------------------------------------------------\n#--------------------------------------------------------------------------------------------------------\n\n\ndef train_and_evaluate_model(hparams,train_ds,val_ds,epochs=30):\n    #åˆ›å»ºæ¨¡å‹\n    model = build_pretrained_model(hparams) \n    \n\n    # åˆ›å»ºå›è°ƒ - è¿™é‡Œçš„ä½œç”¨ç›¸å½“äºåªç”¨åˆ°äº†æ—©åœåˆ¤æ–­\n    training_callback = TrainingCallback(\n        epochs=epochs\n    )\n    \n    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',\n        patience=5,  # éªŒè¯é›†å‡†ç¡®ç‡è¿ç»­5ä¸ªepochæ²¡æœ‰æå‡å°±åœæ­¢\n        min_delta=0.005,\n        restore_best_weights=True, # å…³é”®ï¼šè‡ªåŠ¨æ¢å¤åˆ°æ€§èƒ½æœ€å¥½çš„é‚£ä¸ªepochçš„æƒé‡\n        verbose=0 # åœ¨BOå¾ªç¯ä¸­ä¿æŒå®‰é™\n    )\n    \n    #è®­ç»ƒæ¨¡å‹\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        verbose=0,\n        #batch_size=batch_size,\n        callbacks=[training_callback,early_stopping_callback]\n    )\n\n    loss, accuracy = model.evaluate(val_ds, verbose=0)\n    best_epoch_num = early_stopping_callback.best_epoch + 1\n    print(f\"  > è®­ç»ƒè¯„ä¼°å®Œæˆ: Best Val Acc: {accuracy:.4f}, å¯¹åº”çš„ Loss:{loss:.4f}(at epoch {best_epoch_num})\")\n    \n    return accuracy\n\n\nimport gc \ndef my_objective_fn(x_new):\n    # å°†ä¼˜åŒ–å™¨ç»™å‡ºçš„numpyæ•°ç»„è½¬æ¢ä¸ºè¶…å‚æ•°å­—å…¸\n    hparams = {\n        'lr': x_new[0],\n        'dense_units': int(x_new[1]),\n        'dropout_rate': x_new[2],\n        'l2_reg': x_new[3],\n        'optimizer_choice': int(x_new[4]),\n        'momentum': x_new[5],\n    }\n    # æ¸…ç†ä¼šè¯å¹¶è°ƒç”¨è®­ç»ƒå‡½æ•°\n    tf.keras.backend.clear_session()\n    gc.collect()\n    print(f\"\\næ­£åœ¨è¯„ä¼°è¶…å‚æ•°: {hparams}\")\n    accuracy = train_and_evaluate_model(hparams, train_ds, val_ds, epochs=12)\n    print(f\"è¯„ä¼°å®Œæˆ - éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}\")\n    return accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:13:50.453031Z","iopub.execute_input":"2025-09-15T13:13:50.453371Z","iopub.status.idle":"2025-09-15T13:13:50.470933Z","shell.execute_reply.started":"2025-09-15T13:13:50.453349Z","shell.execute_reply":"2025-09-15T13:13:50.470183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# è´å¶æ–¯ä¼˜åŒ–å…¨è¿‡ç¨‹çš„å›¾åƒè¡¨ç¤º(ä»å¯¼å…¥è´å¶æ–¯è¿­ä»£æ•°æ®å¼€å§‹)\n# ===================================================================\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\ninitial_points_bo=10\n\nlog_file_random = \"/kaggle/input/random-data-25/second_resumed_random_search_25.json\" \nlog_file_BO=\"/kaggle/input/bo-data-30/second_continued_EI_MCMC_BO_log30.json\" \n\n\ndef load_accuracies_from_log(file_path):\n    try:\n        with open(file_path, 'r') as f:\n            logs = [json.loads(line) for line in f if line.strip()]#.stripå»é™¤è¡Œé¦–å°¾çš„ç©ºç™½å­—ç¬¦\n        accuracies = [log['accuracy'] for log in logs]\n        best_log_entry=max(logs,key=lambda entry:entry['accuracy'])\n        best_params=best_log_entry['params']\n        print(f\"âœ… Log file '{file_path}' loaded successfully! Loaded {len(accuracies)} evaluation records.\")\n        return accuracies, best_params\n        \n    except FileNotFoundError:\n        print(f\"âŒ é”™è¯¯: æ—¥å¿—æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚\")\n        return None\n    except Exception as e:\n        print(f\"å¤„ç†æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {e}\")\n        return None\n\naccuracies_random, best_params_random = load_accuracies_from_log(log_file_random)\nbest_idx_random = np.argmax(accuracies_random)\nbest_x_random = best_idx_random + 1\nbest_y_random = accuracies_random[best_idx_random]\n\nif best_params_random:\n    print(\"\\n--- éšæœºæœç´¢æ‰¾åˆ°çš„æœ€ä½³å‚æ•°ç»„åˆ ---\")\n    for key, value in best_params_random.items():\n        if key in ['dense_units', 'optimizer_choice']:\n            print(f\"  - {key}: {int(value)}\")\n        else:\n            print(f\"  - {key}: {value:.4f}\")\n    print(f\"å…¨å±€éšæœºæ¢ç´¢æœ€ä¼˜Val Accuracy:{best_y_random:.4f} at point {best_x_random}\")\n\n\naccuracies_bo, best_params_bo = load_accuracies_from_log(log_file_BO)\nbest_idx_bo = np.argmax(accuracies_bo)\nbest_x_bo=best_idx_bo+1\nbest_y_bo=accuracies_bo[best_idx_bo]\n\nif best_params_bo:\n    if best_idx_bo>initial_points_bo:\n        print(\"\\n--- è´å¶æ–¯ä¼˜åŒ–æ‰¾åˆ°çš„æœ€ä½³å‚æ•°ç»„åˆ ---\")\n        for key, value in best_params_bo.items():\n            if key in ['dense_units', 'optimizer_choice']:\n                print(f\"  - {key}: {int(value)}\")\n            else:\n                print(f\"  - {key}: {value:.4f}\")\n        print(f\"BOé˜¶æ®µæœ€ä¼˜Val Accuracy:{best_y_bo:.4f} at point {best_x_bo}\")\n        absolute_improvement=max(0, best_y_bo-best_y_random)\n        if best_y_random > 0:\n            relative_improvement = (absolute_improvement / best_y_random) * 100\n            print(f\"- Absolute Improvement from Bayesian Opt: +{absolute_improvement:.4f} points\")\n            print(f\"- Relative Improvement from Bayesian Opt: +{relative_improvement:.2f}%\")\n        else:\n            print(f\"- Absolute Improvement from Bayesian Opt: +{absolute_improvement:.4f} points\")\n        print(\"=\"*50)\n        \n    else: \n        print(\"è­¦å‘Šï¼šBO æ²¡æœ‰æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°ç‚¹ï¼\")\n\n#===========================================================================================\n# ç”»å›¾éƒ¨åˆ†\n#===========================================================================================\nif accuracies_random is not None and accuracies_bo is not None:\n    plt.figure(figsize=(16,9))\n    # ---------------- ç»˜åˆ¶çº¯éšæœºæœç´¢çš„æ•°æ® ------------\n    n_evals_random = len(accuracies_random)\n    #åˆ›å»ºxè½´.å¥æ³•np.arangeå³array rangeç¼©å†™,è¾“å…¥æ˜¯(start,stop),å·¦ç«¯ç‚¹åŒ…å«,å³ç«¯ç‚¹ä¸åŒ…å«\n    x_axis_random = np.arange(1, n_evals_random + 1)\n    plt.plot(x_axis_random, accuracies_random, 'o-', color='steelblue', \n             alpha=0.7, label='Single Evaluation (Random Search)')\n    plt.plot(x_axis_random, np.maximum.accumulate(accuracies_random),linestyle='--', \n             color='dodgerblue', linewidth=2.5, \n             label='Progressive Best (Random Search)')\n    \n    plt.plot(best_x_random, best_y_random, \n             marker='*', \n             color='darkblue', \n             markersize=20, \n             markeredgecolor='black',\n             linestyle='', \n             label=f'Global Best Point in random exploration(Accuracy: {best_y_random:.4f})\\nat Evaluation #{best_x_random}')\n\n    # --------- ç»˜åˆ¶è´å¶æ–¯ä¼˜åŒ–çš„æ•°æ® ------------------\n    n_evals_bo = len(accuracies_bo)\n    x_axis_bo = np.arange(1, n_evals_bo + 1)\n    \n    # ç»˜åˆ¶æ¯æ¬¡è¯„ä¼°çš„å‡†ç¡®ç‡ï¼ˆä½œä¸ºå‚è€ƒï¼‰\n    plt.plot(x_axis_bo, accuracies_bo, 's-', color='lightcoral', \n             alpha=0.8, label='Single Evaluation (Bayesian Opt.)')\n    # ç»˜åˆ¶ç´¯ç§¯æœ€ä½³å‡†ç¡®ç‡æ›²çº¿\n    plt.plot(x_axis_bo, np.maximum.accumulate(accuracies_bo),linestyle='--', \n             color='darkred', linewidth=2.5, \n             label='Progressive Best (Bayesian Opt.)')\n\n    plt.plot(best_x_bo, best_y_bo, \n             marker='*', \n             color='firebrick', #æœ€æ·±çš„çº¢è‰²\n             markersize=20, \n             markeredgecolor='black',\n             linestyle='', # No connecting line\n             label=f'Global Best Point in BO(Accuracy: {best_y_bo:.4f})\\nat Evaluation #{best_x_bo}')\n    \n    # åœ¨BOæ›²çº¿ä¸Šæ ‡è®°éšæœºæ¢ç´¢ç»“æŸçš„ä½ç½®\n    if n_evals_bo > initial_points_bo:\n        plt.axvline(initial_points_bo, color='gray', linestyle=':', linewidth=2, \n                    label=f'End of {initial_points_bo} Random Explorations for BO')\n\n    # --- æ·»åŠ å›¾è¡¨å…ƒç´  ---\n    plt.title('Performance Comparison: Random Search vs. Bayesian Optimization', fontsize=18)\n    plt.xlabel('Number of Iteration', fontsize=14)\n    plt.ylabel('Validation Accuracy', fontsize=14)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    #=============================================================\n    # 1. è®¾ç½®Xè½´çš„è¾¹ç•Œ\n    plt.xlim(0, 25)\n    # 2. è®¾ç½®Xè½´çš„åˆ»åº¦é—´éš”ä¸º1\n    # plt.xticks(np.arange(0, 26, 1))\n\n    plt.legend(fontsize=12, loc='lower right')\n    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n    plt.tight_layout()\n\n    # ä¿å­˜å¹¶æ˜¾ç¤ºå›¾åƒ\n    plt.savefig('comparison_random_vs_bo22.png')\n    plt.show()\n\nelse:\n    print(\"\\næœªèƒ½æˆåŠŸåŠ è½½æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆæ¯”è¾ƒå›¾ã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T13:38:03.087743Z","iopub.execute_input":"2025-09-16T13:38:03.088521Z","iopub.status.idle":"2025-09-16T13:38:03.706573Z","shell.execute_reply.started":"2025-09-16T13:38:03.088487Z","shell.execute_reply":"2025-09-16T13:38:03.70573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ç»˜åˆ¶å•BOè¿‡ç¨‹çš„ä¼˜åŒ–è·¯å¾„\nplt.figure(figsize=(10, 6))\nplt.plot(range(len(accuracies_bo)), accuracies_bo, 'o-', label='Current Best')\n# plt.plot(range(len(accuracies_bo)), [max(accuracies_bo[:i+1]) for i in range(len(accuracies_bo))], \n#          'r--', label='Progressive Best')\nplt.plot(range(len(accuracies_bo)), np.maximum.accumulate(accuracies_bo), \n         'r--', label='Progressive Best')\nplt.xlabel('Number of Iteration')\nplt.ylabel('Validation Accuracy')\nplt.title('Bayesian Optimization Progress')\nplt.legend()\nplt.grid(True)\nplt.savefig('bo_30.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T13:37:21.334503Z","iopub.execute_input":"2025-09-16T13:37:21.334866Z","iopub.status.idle":"2025-09-16T13:37:21.628466Z","shell.execute_reply.started":"2025-09-16T13:37:21.334844Z","shell.execute_reply":"2025-09-16T13:37:21.627781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# å† å†›å¯¹å†³ (Champion Showdown)\n# ===================================================================\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\nimport time\nimport numpy as np\n\n\n\n# ===================================================================\n# å®šä¹‰ç”¨äºåŠ è½½å’Œæ‰¾å‡ºä¸‰ç»„å† å†›è¶…å‚æ•°çš„å‡½æ•°\n# ===================================================================\ndef get_champion_hyperparameters(log_file_random, log_file_bo, init_points_bo=5):\n    \"\"\"\n    ä»ä¸¤ä¸ªæ—¥å¿—æ–‡ä»¶ä¸­åŠ è½½ç»“æœï¼Œå¹¶æ‰¾å‡ºä¸‰ç»„å† å†›è¶…å‚æ•°ã€‚\n    \"\"\"\n    \n    def load_full_logs(file_path):\n        \"\"\"è¾…åŠ©å‡½æ•°ï¼Œç”¨äºåŠ è½½å®Œæ•´çš„æ—¥å¿—æ¡ç›®ã€‚\"\"\"\n        try:\n            with open(file_path, 'r') as f:\n                return [json.loads(line) for line in f if line.strip()]\n        except FileNotFoundError:\n            print(f\"âŒ é”™è¯¯: æ—¥å¿—æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚\")\n            return None\n\n    # --- åŠ è½½ä¸¤ä¸ªæ—¥å¿—æ–‡ä»¶çš„å…¨éƒ¨æ•°æ® ---\n    logs_random = load_full_logs(log_file_random)\n    logs_bo = load_full_logs(log_file_bo)\n\n    if not logs_random or not logs_bo:\n        print(\"ç¼ºå°‘å¿…è¦çš„æ—¥å¿—æ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­ã€‚\")\n        return None\n\n    # --- 1. æ‰¾å‡ºéšæœºæ¢ç´¢å† å†› ---\n    best_random_entry = max(logs_random, key=lambda log: log['accuracy'])\n    random_champion_params = best_random_entry['params']\n\n    # --- 2. æ‰¾å‡ºè´å¶æ–¯ä¼˜åŒ–é˜¶æ®µçš„å† å†› ---\n    # åªåœ¨ç¬¬ `init_points_bo` ä¸ªç‚¹ä¹‹åçš„éƒ¨åˆ†è¿›è¡Œæœç´¢\n    bo_phase_logs = logs_bo[init_points_bo:]\n    if bo_phase_logs:\n        best_bo_entry = max(bo_phase_logs, key=lambda log: log['accuracy'])\n        bo_champion_params = best_bo_entry['params']\n    else:\n        print(\"è­¦å‘Š: è´å¶æ–¯ä¼˜åŒ–é˜¶æ®µæ²¡æœ‰æ•°æ®ç‚¹ï¼Œå°†è·³è¿‡æ­¤å† å†›ã€‚\")\n        bo_champion_params = {}\n\n    # --- 3. å®šä¹‰äººç±»åŸºå‡†å† å†› ---\n    manual_baseline_params = {\n        'lr': 0.00005,\n        'dense_units': 256.0,\n        'dropout_rate': 0.5,\n        'l2_reg': 0.001,\n        'optimizer_choice': 0, # Adam\n        'momentum': 0.9      # Adamä¸ç›´æ¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œä½†ä¸ºç»“æ„ç»Ÿä¸€è€Œä¿ç•™\n    }\n    \n    champions = {\n        \"Human Baseline\": manual_baseline_params,\n        \"Random Search Champion\": random_champion_params,\n        \"BO Champion\": bo_champion_params,\n    }\n    \n    # æ¸…ç†ç©ºæ¡ç›®å¹¶è½¬æ¢å‚æ•°ç±»å‹\n    final_champions = {}\n    for name, params in champions.items():\n        if params: # å¦‚æœå‚æ•°å­—å…¸ä¸ä¸ºç©º\n            if 'dense_units' in params:\n                params['dense_units'] = int(params['dense_units'])\n            if 'optimizer_choice' in params:\n                params['optimizer_choice'] = int(params['optimizer_choice'])\n            final_champions[name] = params\n\n    print(\"--- å‘ç°ä»¥ä¸‹å† å†›è¶…å‚æ•°è¿›è¡Œå¯¹å†³ ---\")\n    for name, params in final_champions.items():\n        print(f\"\\nğŸ† {name}:\")\n        for key, value in params.items():\n            if isinstance(value, int):\n                print(f\"   - {key}: {value}\")\n            else:\n                print(f\"   - {key}: {float(value):.6f}\")\n            \n    return final_champions\n\n# ===================================================================\n# å®šä¹‰æœ€ç»ˆçš„è®­ç»ƒä¸è¯„ä¼°å‡½æ•° \n# ===================================================================\ndef train_and_evaluate_champion(hparams, train_ds, val_ds, test_ds):\n    tf.keras.backend.clear_session(); gc.collect()\n    \n    # ç¡®ä¿ä¼ å…¥çš„hparamsä¸­çš„optimizer_choiceæ˜¯æ•´æ•°\n    if 'optimizer_choice' in hparams:\n        hparams['optimizer_choice'] = int(hparams['optimizer_choice'])\n        \n    model = build_pretrained_model(hparams)\n    \n    # ä¸ºæœ€ç»ˆå¯¹å†³è®¾ç½®æ›´é•¿çš„è®­ç»ƒå‘¨æœŸå’Œè€å¿ƒå€¼\n    MAX_EPOCHS = 100\n    PATIENCE = 10\n\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy', \n        patience=PATIENCE, \n        min_delta=0.001, \n        restore_best_weights=True, \n        verbose=1)\n    \n    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n    \n    logging_callback = TrainingCallback(epochs=MAX_EPOCHS)\n    \n    start_time = time.time()\n    history = model.fit(\n        train_ds, \n        validation_data=val_ds, \n        epochs=MAX_EPOCHS, \n        callbacks=[logging_callback, early_stopping, lr_scheduler], \n        verbose=0)\n    training_duration = time.time() - start_time\n    \n    print(\"\\n--- æ­£åœ¨è¿›è¡Œæœ€ç»ˆè¯„ä¼° ---\")\n    val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n    best_epoch = early_stopping.best_epoch + 1 if early_stopping.best_epoch >= 0 else len(history.history['loss'])\n\n    # è¿”å›åŒ…å«æµ‹è¯•é›†æ€§èƒ½çš„ç»“æœ\n    return {\n        \"val_accuracy\": val_acc,\n        \"test_accuracy\": test_acc,\n        \"best_epoch\": best_epoch,\n        \"history\": history.history,\n        \"training_time\": training_duration\n    }\n    \n# ===================================================================\n# ä¸»æ‰§è¡Œæµç¨‹\n# ===================================================================\n# å®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„\n\nlog_file_random = \"/kaggle/input/random-data-25/second_resumed_random_search_25.json\" \nlog_file_bo=\"/kaggle/input/bo-data-30/second_continued_EI_MCMC_BO_log30.json\" \n\n# è·å–ä¸‰ç»„å† å†›å‚æ•°\nchampions_to_train = get_champion_hyperparameters(\n    log_file_random=log_file_random,\n    log_file_bo=log_file_bo,\n    init_points_bo=10\n)\n\nfinal_results = {}\ntraining_histories = {}\nif champions_to_train:\n    for name, params in champions_to_train.items():\n        print(f\"\\n{'='*60}\\nğŸ¥Š å¼€å§‹æœ€ç»ˆè®­ç»ƒ: {name}\\n{'='*60}\")\n        \n        # ä¸ºæ¯æ¬¡å¯¹å†³é‡ç½®éšæœºç§å­ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ\n        # set_reproducibility(seed=SEED_VALUE) \n        \n        results = train_and_evaluate_champion(params, train_ds, val_ds, test_ds)\n        \n        final_results[name] = results\n        training_histories[name] = results['history']\n        \n        print(f\"âœ… ç»„åˆ {name} è®­ç»ƒå®Œæˆ!\")\n        print(f\"   - æ”¶æ•›äºè½®æ¬¡ (Converged at Epoch): {results['best_epoch']}\")\n        print(f\"Â  Â - éªŒè¯é›†å‡†ç¡®ç‡ (Validation Acc): {results['val_accuracy']:.4f}\")\n        print(f\"   - æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡ (Test Acc): {results['test_accuracy']:.4f}\")\n        print(f\"   - è®­ç»ƒè€—æ—¶: {results['training_time'] / 60:.2f} åˆ†é’Ÿ\")\n\n# ===================================================================\n# ç»“æœå¯è§†åŒ–\n# ===================================================================\ndef format_time(seconds):\n    hours, remainder = divmod(int(seconds), 3600)\n    minutes, secs = divmod(remainder, 60)\n    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n\nif final_results:\n    print(f\"\\n{'='*60}\\nğŸ† å† å†›å¯¹å†³æœ€ç»ˆç»“æœ ğŸ†\\n{'='*60}\")\n    \n    results_df = pd.DataFrame({\n        'Champion': list(final_results.keys()),\n        'Validation Accuracy': [res['val_accuracy'] for res in final_results.values()],\n        'Test Accuracy': [res['test_accuracy'] for res in final_results.values()],\n        'Converged at Epoch': [res['best_epoch'] for res in final_results.values()],\n        'Training Time': [format_time(res['training_time']) for res in final_results.values()]\n    }).sort_values(by='Test Accuracy', ascending=False).reset_index(drop=True)\n\n    print(results_df.to_string())\n    \n    # ä¿å­˜ç»“æœåˆ°CSV\n    results_df.to_csv(\"champion_showdown_final_results.csv\", index=False)\n    print(\"\\n[INFO] æœ€ç»ˆå¯¹å†³ç»“æœå·²ä¿å­˜åˆ° champion_showdown_final_results.csv\")\n\n    winner = results_df.iloc[0]\n    print(f\"\\nğŸ‰ æ€»å† å†›æ˜¯: {winner['Champion']}, æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡ä¸º {winner['Test Accuracy']:.4f}!\")\n\n    # ç»˜åˆ¶éªŒè¯é›†å‡†ç¡®ç‡æ›²çº¿è¿›è¡Œæ¯”è¾ƒ\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(12, 8))\n    for name, history in training_histories.items():\n        plt.plot(history['val_accuracy'], label=f'{name} Val Accuracy', lw=2, marker='o', markersize=4, alpha=0.8)\n\n    plt.title('Champion PK: Curve of Validation accuracy', fontsize=16)\n    plt.xlabel('Epoch', fontsize=12)\n    plt.ylabel('Validation Accuracy', fontsize=12)\n    plt.legend(fontsize=12)\n    plt.grid(True)\n    plt.savefig('championship_curves.png')\n    plt.show()\nelse:\n    print(\"\\næœªèƒ½ç¡®å®šå† å†›ç»„åˆï¼Œæ— æ³•å¼€å§‹å¯¹å†³ã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:19:59.018899Z","iopub.execute_input":"2025-09-15T13:19:59.01921Z","iopub.status.idle":"2025-09-15T14:28:13.4236Z","shell.execute_reply.started":"2025-09-15T13:19:59.019184Z","shell.execute_reply":"2025-09-15T14:28:13.422918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# ä¿å­˜è¯¦ç»†çš„è®­ç»ƒå†å²æ•°æ®ä¸º JSON æ–‡ä»¶\n# ===================================================================\n# æ£€æŸ¥ 'training_histories' å˜é‡æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º\nif 'training_histories' in locals() and training_histories:\n    \n    history_json_path = 'champion_training_histories.json'\n    \n    try:\n        # ä»¥å†™å…¥æ¨¡å¼('w')æ‰“å¼€æ–‡ä»¶\n        with open(history_json_path, 'w') as f:\n            # ä½¿ç”¨ json.dump å°†å­—å…¸å†™å…¥æ–‡ä»¶\n            json.dump(training_histories, f, indent=4)\n            \n        print(f\"\\n[INFO] âœ… å…¨éƒ¨ä¸‰ç»„å† å†›çš„è¯¦ç»†è®­ç»ƒå†å²å·²æˆåŠŸä¿å­˜åˆ°: {history_json_path}\")\n        \n    except Exception as e:\n        print(f\"\\n[ERROR] âŒ ä¿å­˜JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if final_results:\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n    \n    colors = {\n        \"Human Baseline\": \"C0\", # è“è‰²\n        \"Random Search Champion\": \"C1\", # æ©™è‰²\n        \"BO Champion\": \"C2\"   # ç»¿è‰²\n    }\n\n    # éå†æ¯ä¸ªå† å†›çš„è®­ç»ƒå†å²\n    for name, history in training_histories.items():\n        # æå–å„ä¸ªæŒ‡æ ‡çš„å†å²æ•°æ®\n        loss_history = history['loss']\n        acc_history = history['accuracy']\n        val_loss_history = history['val_loss']\n        val_acc_history = history['val_accuracy']\n        \n        # åˆ›å»ºä»1å¼€å§‹çš„Epoch Xè½´\n        epochs = range(1, len(loss_history) + 1)\n\n        # --- åœ¨å¯¹åº”çš„å­å›¾ä¸Šç»˜åˆ¶æ›²çº¿ ---\n        \n        # 1. å·¦ä¸Šè§’ï¼šè®­ç»ƒé›†æŸå¤± (Training Loss)\n        axes[0, 0].plot(epochs, loss_history, label=name, color=colors.get(name), lw=2, marker='.', markersize=5, alpha=0.8)\n        \n        # 2. å³ä¸Šè§’ï¼šè®­ç»ƒé›†å‡†ç¡®ç‡ (Training Accuracy)\n        axes[0, 1].plot(epochs, acc_history, label=name, color=colors.get(name), lw=2, marker='.', markersize=5, alpha=0.8)\n        \n        # 3. å·¦ä¸‹è§’ï¼šéªŒè¯é›†æŸå¤± (Validation Loss)\n        axes[1, 0].plot(epochs, val_loss_history, label=name, color=colors.get(name), lw=2, marker='.', markersize=5, alpha=0.8)\n        \n        # 4. å³ä¸‹è§’ï¼šéªŒè¯é›†å‡†ç¡®ç‡ (Validation Accuracy)\n        axes[1, 1].plot(epochs, val_acc_history, label=name, color=colors.get(name), lw=2, marker='.', markersize=5, alpha=0.8)\n\n    # è®¾ç½®æ ‡é¢˜\n    axes[0, 0].set_title('Training Loss vs. Epoch', fontsize=14)\n    axes[0, 1].set_title('Training Accuracy vs. Epoch', fontsize=14)\n    axes[1, 0].set_title('Validation Loss vs. Epoch', fontsize=14)\n    axes[1, 1].set_title('Validation Accuracy vs. Epoch', fontsize=14)\n\n    # å¾ªç¯ä¸ºæ¯ä¸ªå­å›¾è®¾ç½®æ ‡ç­¾ã€å›¾ä¾‹å’Œç½‘æ ¼\n    for i in range(2):\n        for j in range(2):\n            axes[i, j].set_xlabel('Epoch', fontsize=12)\n            axes[i, j].legend(fontsize=10)\n            axes[i, j].grid(True)\n            # ç¡®ä¿Xè½´åˆ»åº¦ä¸ºæ•´æ•°\n            ax = axes[i, j]\n            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n            ax.set_xlim(left=1)\n\n    # å•ç‹¬è®¾ç½®Yè½´æ ‡ç­¾\n    axes[0, 0].set_ylabel('Loss', fontsize=12)\n    axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n    axes[1, 0].set_ylabel('Loss', fontsize=12)\n    axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n    \n    # # èšç„¦å‡†ç¡®ç‡æ›²çº¿çš„é«˜åˆ†åŒº\n    # axes[0, 1].set_ylim(bottom=0.90) \n    # axes[1, 1].set_ylim(bottom=max(0.85, results_df['Validation Accuracy'].min() - 0.02))\n\n    # æ€»æ ‡é¢˜\n    fig.suptitle('Training Dynamics Analysis', fontsize=20)\n    \n    # è°ƒæ•´å­å›¾å¸ƒå±€ï¼Œé˜²æ­¢æ ‡é¢˜å’Œæ ‡ç­¾é‡å \n    fig.tight_layout(rect=[0, 0, 1, 0.96]) # rectä¸ºæ€»æ ‡é¢˜ç•™å‡ºç©ºé—´\n\n    # ä¿å­˜å¹¶æ˜¾ç¤ºå›¾åƒ\n    plt.savefig('championship_match_details_2x2.png')\n    plt.show()\n\nelse:\n    print(\"\\næœªèƒ½æ‰§è¡Œæœ€ç»ˆå¯¹å†³ï¼Œè·³è¿‡ç”Ÿæˆ 2x2 æ›²çº¿å›¾ã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:32:26.16562Z","iopub.execute_input":"2025-09-15T14:32:26.165952Z","iopub.status.idle":"2025-09-15T14:32:28.277191Z","shell.execute_reply.started":"2025-09-15T14:32:26.165934Z","shell.execute_reply":"2025-09-15T14:32:28.276144Z"}},"outputs":[],"execution_count":null}]}