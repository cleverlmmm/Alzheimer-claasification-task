{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12107936,"sourceType":"datasetVersion","datasetId":7623043}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\nver1--------------------------------\n1.用了自定义的GP_MCMC方法和积分型取平均的EI采集函数\n2.和EI_MCMC_random是对标版本\n3.去掉了trainningcallback中的交互选项，所以此版本适用于后台运行\n\nver4----------------------------------\nepoch=40，但设置了早停，连续五次没提升就早停\n10次随机探索+15次BO\n","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport time\nimport shutil\nimport pathlib\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\n\n\nfrom tensorflow import keras\nfrom keras import models, optimizers, metrics, layers, regularizers, losses\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad\nimport json\n\ndef set_reproducibility(seed=88):\n    \"\"\"\n    设置一个全局种子来确保整个实验流程的可复现性。\n    这包括Python内置random、os、NumPy和TensorFlow。\n    同时，它会尝试配置TensorFlow使用确定性算法。\n    \n    Args:\n        seed (int): 您选择的随机种子。\n    \"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.config.experimental.enable_op_determinism()\n    \n    print(f\"✅ 全局随机种子已设置为: {seed}，并已启用TensorFlow确定性操作。\")\n\n\nSEED_VALUE = 88 \nset_reproducibility(SEED_VALUE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:29:04.949917Z","iopub.execute_input":"2025-09-04T10:29:04.950118Z","iopub.status.idle":"2025-09-04T10:29:29.624995Z","shell.execute_reply.started":"2025-09-04T10:29:04.9501Z","shell.execute_reply":"2025-09-04T10:29:29.624235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#数据集会自动解压缩，只需要直接讲数据加载到output中\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom tensorflow import keras\nfrom keras import layers\nimport matplotlib.image as img\n\n\n'''加载到output路径并划分文件夹'''\n\n!pip install split-folders -q\nimport splitfolders\ninput_dir=\"/kaggle/input/augmented-alzheimer-data/augmented_data\"\noutput_dir=\"./output\"\nsplitfolders.ratio(\n    input_dir, \n    output=output_dir, \n    seed=1345, \n    ratio=(.70, 0.15,0.15),\n    group_prefix=None \n) \nprint(\"Dataset split completed!\")\n\n#验证输出路径文件夹文件数量\ndef count_files(directory):\n    for split in ['train', 'val', 'test']:\n        split_path = os.path.join(directory, split)\n        print(f\"\\n{split.upper()} set:\")\n        for class_name in os.listdir(split_path):\n            class_path = os.path.join(split_path, class_name)\n            num_files = len(os.listdir(class_path))\n            print(f\"  {class_name}: {num_files} images\")\n\ncount_files(\"./output\")\n\nAUTOTUNE = tf.data.AUTOTUNE\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nbatch_size=32\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/train\",\nseed=123,\nlabel_mode='int',#这是默认值\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=True\n).prefetch(buffer_size=AUTOTUNE)\n# 默认情况下label_mode='int'后续匹配sparse_categorical_crossentropy\t需设置from_logits=True/False\n\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/test\",\nlabel_mode='int',\nseed=123,\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=False  \n).prefetch(buffer_size=AUTOTUNE)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\"./output/val\",\nlabel_mode='int',\nseed=123,\nimage_size=(IMG_HEIGHT, IMG_WIDTH),\nbatch_size=batch_size,\nshuffle=False\n).prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:29:29.625664Z","iopub.execute_input":"2025-09-04T10:29:29.626196Z","iopub.status.idle":"2025-09-04T10:32:09.160691Z","shell.execute_reply.started":"2025-09-04T10:29:29.626177Z","shell.execute_reply":"2025-09-04T10:32:09.159864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, time, gc, json\nimport numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nimport tensorflow as tf, tensorflow_probability as tfp\nfrom tensorflow import keras\nfrom keras import layers, regularizers\nimport scipy.stats as stats\ntfd = tfp.distributions\nsns.set_style('whitegrid')\ntf.keras.backend.set_floatx('float64')\n\n#-----------------------------模型架构搭建：顺序模型------------------------------\n\ndef build_pretrained_model(hparams):\n    # 1. 加载预训练的EfficientNet，不包含顶部的分类层\n    base_model = tf.keras.applications.EfficientNetB0(\n        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    # 2. 冻结预训练模型的权重\n    base_model.trainable = False\n    \n    # 3. 确定从哪个块开始解冻（微调）\n    fine_tune_at_block = 'block5a_expand_conv'\n    # 3. 遍历所有层，解冻 fine_tune_at_block 及之后的所有层\n    set_trainable = False\n    for layer in base_model.layers:\n        if layer.name == fine_tune_at_block:\n            set_trainable = True\n        if set_trainable:\n            layer.trainable = True\n        else:\n            layer.trainable = False\n\n    # 最佳实践：无论是否解冻，都建议保持 BatchNormalization 层处于冻结状态\n    # 在微调时，用小批量数据更新BN层的统计数据可能会引入噪声，破坏预训练学到的分布\n    for layer in base_model.layers:\n        if isinstance(layer, layers.BatchNormalization):\n            layer.trainable = False\n    \n    # # --- 核心修改 1: 解冻模型的最后20层 (在微调时，通常保持 BatchNormalization 层被冻结，这有助于稳定训练)\n    # for layer in base_model.layers[-20:]:\n    #     if not isinstance(layer, layers.BatchNormalization):\n    #         layer.trainable = True\n            \n    dense_units = int(hparams.get('dense_units', 128))\n    dropout_rate = hparams.get('dropout_rate', 0.5)\n    l2_reg = hparams.get('l2_reg', 0.001)\n    lr = hparams.get('lr', 0.0001)\n    optimizer_choice = int(hparams.get('optimizer_choice', 0))\n    momentum = hparams.get('momentum', 0.9)\n    \n    \n\n\n    # 3. 在预训练模型之上，构建一个【小而精】的分类头\n    model = tf.keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(), # 使用全局平均池化替代Flatten，参数更少，不易过拟合\n        layers.Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)), # 使用更小的L2值\n        layers.Dropout(dropout_rate),\n        layers.Dense(4, activation='softmax') # 4个类别\n    ])\n\n \n    if optimizer_choice == 0:\n        print(\"--- 使用 Adam 优化器 ---\")\n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    elif optimizer_choice == 1:\n        print(f\"--- 使用 SGD 优化器 (momentum={momentum:.3f}) ---\")\n        optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n    elif optimizer_choice == 2:\n        print(\"--- 使用 RMSprop 优化器 ---\")\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n    elif optimizer_choice ==3:\n        print(\"--- 使用 Adagrad 优化器 ---\")\n        optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n    else: # optimizer_choice == 分数\n        print(\"--- 无法找到对应的优化器，请检查optimizer_choice value ---\")\n        \n\n    \n    model.compile(\n        optimizer=optimizer,\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n   \n    return model\n\n\n\n#---------------------------------------------------------------------------------------------------------\nclass TrainingCallback(keras.callbacks.Callback):\n    def __init__(self, epochs):\n        super().__init__()\n        self.epochs = epochs\n    def on_train_begin(self, logs=None):\n        header = \"{0:^8s}{1:^11s}{2:^11s}{3:^12s}{4:^12s}{5:^12s}\".format('Epoch', 'Loss', 'Acc', 'Val_Loss', 'Val_Acc', 'Duration')\n        print(header)\n    def on_epoch_begin(self, epoch, logs=None): \n        self.ep_start = time.time()\n    def on_epoch_end(self, epoch, logs=None):\n        duration = time.time() - self.ep_start\n        acc = logs.get('accuracy', 0); val_acc = logs.get('val_accuracy', 0)\n        loss = logs.get('loss', 0); val_loss = logs.get('val_loss', 0)\n        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^10.3f}{acc * 100:^11.3f}{val_loss:^12.5f}{val_acc * 100:^12.3f}{duration:^12.2f}'\n        print(msg)\n        \n\n#--------------------------------------------------------------------------------------------------------\n\n\ndef train_and_evaluate_model(hparams,train_ds,val_ds,epochs=30):\n    #创建模型\n    model = build_pretrained_model(hparams) \n    \n\n    training_callback = TrainingCallback(\n        epochs=epochs\n    )\n    \n    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',\n        patience=5,  # 验证集准确率连续5个epoch没有提升就停止\n        min_delta=0.005,\n        restore_best_weights=True, # 关键：自动恢复到性能最好的那个epoch的权重\n        verbose=0 # 在BO循环中保持安静\n    )\n    \n    #训练模型\n    history = model.fit(\n        train_ds,#指定训练数据集(必需参数)\n         #验证数据集(可选参数)在每个epoch结束后，模型会在这个数据集上评估性能\n        validation_data=val_ds,\n        epochs=epochs,#遍历训练数据的总轮次(必需参数)\n        verbose=0,#日志显示模式,设置为0时,通常是因为使用了自定义回调函数来显示更详细的信息\n        #batch_size=batch_size,\n        callbacks=[training_callback,early_stopping_callback]\n    )\n\n    # 返回验证集准确率（贝叶斯优化会最大化此值）\n    loss, accuracy = model.evaluate(val_ds, verbose=0)\n    # 我们可以从早停回调中获取最佳轮次信息\n    best_epoch_num = early_stopping_callback.best_epoch + 1\n    print(f\"  > 训练评估完成: Best Val Acc: {accuracy:.4f}, 对应的 Loss:{loss:.4f}(at epoch {best_epoch_num})\")\n    \n    return accuracy\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:36:40.344504Z","iopub.execute_input":"2025-09-04T10:36:40.344845Z","iopub.status.idle":"2025-09-04T10:58:06.701368Z","shell.execute_reply.started":"2025-09-04T10:36:40.344818Z","shell.execute_reply":"2025-09-04T10:58:06.699962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BayesianOptimizer:\n    def __init__(self, search_space, init_points=5, exploration=0.01, \n                 log_file=\"custom_bo_log.json\", initial_X=None, initial_y=None):\n        self.search_space = np.array(search_space)\n        self.dim = len(search_space)\n        self.init_points = init_points\n        self.exploration = exploration\n        self.log_file = log_file\n        #----以防训练由于超时中断,设置自断点处接着BO迭代,加载已计算好的数据\n        if initial_X is not None and initial_y is not None:\n            self.X_obs = initial_X\n            self.y_obs = initial_y\n            print(f\"BO optimizer has used {len(self.y_obs)} observed datas to initialize\")\n            #日志回填\n            print(f\"new log '{self.log_file}' is set and loads old data into it \")\n            if os.path.exists(self.log_file):\n                os.remove(self.log_file)\n            for i in range(len(self.y_obs)):\n                self._write_log(iteration=i + 1, x_new=self.X_obs[i], y_new=self.y_obs[i])\n            print(f\"历史数据回填完成。共写入 {len(self.y_obs)} 条记录。\")\n        else: \n            self.X_obs = np.zeros((0, self.dim))\n            self.y_obs = np.zeros(0)\n            if os.path.exists(self.log_file):\n                os.remove(self.log_file)\n                \n        self.theta_dim = self.dim + 2 # length_scales (dim) + amplitude + noise\n        \n        self.num_samples = 20\n        self.burn_in = 100\n        \n    \n        \n    def random_sample(self):\n        \"\"\"在搜索空间内随机采样一个超参数点。\"\"\"\n        return np.array([np.random.uniform(low, high) for (low, high) in self.search_space])\n\n    @tf.function(jit_compile=True) \n    def matern52_kernel(self, X1, X2, theta):\n        X1_tf = tf.cast(X1, tf.float64)\n        X2_tf = tf.cast(X2, tf.float64)\n        theta = tf.cast(theta, tf.float64)\n        \n        length_scales = theta[:self.dim]\n        amplitude = theta[self.dim]\n        \n        X1_scaled = X1_tf / length_scales\n        X2_scaled = X2_tf / length_scales\n\n        \n        r2 = tf.reduce_sum(tf.square(X1_scaled), axis=1, keepdims=True) - 2 * tf.matmul(X1_scaled, X2_scaled, transpose_b=True) + tf.reduce_sum(tf.square(X2_scaled), axis=1)\n        r2 = tf.maximum(r2, 1e-12)\n        r = tf.sqrt(r2)\n        \n        # --- 关键修改 ---\n        # 将所有Python字面量常数转换为tf.float64类型的张量\n        five_64 = tf.constant(5.0, dtype=tf.float64)\n        three_64 = tf.constant(3.0, dtype=tf.float64)\n        one_64 = tf.constant(1.0, dtype=tf.float64)\n        \n        sqrt5_64 = tf.sqrt(five_64)\n        \n        # 使用我们新定义的tf.float64常数进行计算\n        term = (one_64 + sqrt5_64 * r + (five_64 / three_64) * r2) * tf.exp(-sqrt5_64 * r)\n        \n        K = amplitude**2 * term\n        return K\n\n    \n    @tf.function(jit_compile=True) \n    def log_posterior(self, theta, X, y): \n        f64 = tf.constant(1e-6, dtype=tf.float64)\n        two_pi_64 = tf.constant(2.0 * np.pi, dtype=tf.float64)\n        half_64 = tf.constant(0.5, dtype=tf.float64)\n        \n        X_tf = tf.cast(X, dtype=tf.float64)\n        y_tf = tf.cast(y, dtype=tf.float64)\n        theta_tf = tf.cast(theta, dtype=tf.float64)\n        \n        noise = theta_tf[self.dim + 1]**2\n        \n        K = self.matern52_kernel(X_tf, X_tf, theta_tf)\n        n = tf.shape(K)[0]\n        K += (noise + f64) * tf.eye(n, dtype=tf.float64)\n        \n        L = tf.linalg.cholesky(K)\n        alpha = tf.linalg.cholesky_solve(L, tf.expand_dims(y_tf, 1))\n        \n        log_lik = -half_64 * tf.squeeze(tf.matmul(tf.expand_dims(y_tf, 0), alpha))\n        log_lik -= tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)))\n        n_64 = tf.cast(n, tf.float64)\n        log_lik -= half_64 * n_64 * tf.math.log(two_pi_64)\n        \n        return log_lik\n\n\n    def sample_theta_posterior(self):\n        def target_log_prob_fn(theta):\n            return self.log_posterior(theta, self.X_obs, self.y_obs)\n\n        kernel = tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=target_log_prob_fn,\n            num_leapfrog_steps=3,\n            step_size=0.1)\n        \n        samples = tfp.mcmc.sample_chain(\n            num_results=self.num_samples,\n            current_state=tf.ones(self.theta_dim, dtype=tf.float64),\n            kernel=kernel,\n            num_burnin_steps=self.burn_in,\n            trace_fn=None)\n        \n        return samples.numpy()\n    \n    def expected_improvement(self, x_candidate, theta_samples):\n        if self.y_obs.size == 0: return 1.0\n        \n        y_best = np.max(self.y_obs)\n        x_candidate_reshaped = x_candidate.reshape(1, -1)\n        \n        ei_values = []\n        for theta in theta_samples:\n\n            try:\n                theta_np = theta.numpy() if hasattr(theta, 'numpy') else theta\n                \n                K_obs = self.matern52_kernel(self.X_obs, self.X_obs, theta_np).numpy()\n                noise = theta_np[self.dim + 1]**2\n                K_obs += (noise + 1e-6) * np.eye(len(self.X_obs))\n                \n                K_cross = self.matern52_kernel(self.X_obs, x_candidate_reshaped, theta_np).numpy()\n                K_candidate = self.matern52_kernel(x_candidate_reshaped, x_candidate_reshaped, theta_np).numpy()\n                L = np.linalg.cholesky(K_obs)\n                L_inv_K_cross = np.linalg.solve(L, K_cross)\n                \n                mu = np.dot(L_inv_K_cross.T, np.linalg.solve(L, self.y_obs))\n                sigma2 = K_candidate - np.dot(L_inv_K_cross.T, L_inv_K_cross)\n\n                # --- 核心修改：安全地将 mu 和 sigma2 转换为标量 ---\n                # mu 是 (1,) 形状的一维数组, 用 [0] 提取\n                mu_scalar = mu[0]\n                # sigma2 是 (1,1) 形状的二维数组, 用 [0,0] 提取\n                sigma2_scalar = sigma2[0, 0]\n                \n                sigma = np.sqrt(np.maximum(sigma2_scalar, 1e-9))\n                # 如果 sigma 过小，则此点没有不确定性，EI为0\n                if sigma == 0.0:\n                    continue\n\n                z = (mu_scalar - y_best - self.exploration) / sigma\n                \n                norm = stats.norm(loc=0, scale=1)\n                ei = (mu_scalar - y_best - self.exploration) * norm.cdf(z) + sigma * norm.pdf(z)\n                \n                # 确保 ei 是一个正常的浮点数\n                if not np.isnan(ei):\n                    ei_values.append(ei)\n\n            except np.linalg.LinAlgError:\n                # 矩阵计算出错时跳过此样本\n                print(\"警告: 在EI计算中发生矩阵奇异值错误，跳过此theta样本。\")\n                continue\n                \n        return np.mean(ei_values) if ei_values else 0.0\n         \n\n    \n    def _write_log(self, iteration, x_new, y_new):\n        param_names = ['lr','dense_units','dropout_rate','l2_reg','optimizer_choice','momentum']\n        params_dict = {name:val for name, val in zip(param_names, x_new)}\n        log_entry={'iteration':int(iteration),\n                   'accuracy':float(y_new),\n                   'params':params_dict}\n        with open(self.log_file,'a') as f: \n            f.write(json.dumps(log_entry)+'\\n')\n    \n    def optimize(self, objective_fn, n_iter=30):\n        #检查是 初次BO 还是 接续BO\n        num_existing_points=self.X_obs.shape[0]\n        num_init_points_to_run = max(0, self.init_points - num_existing_points)\n\n        if num_init_points_to_run > 0:\n            # 场景A: 加载的点数少于要求的初始随机化点数，需要补充运行\n            print(f\"--- 继续随机初始化阶段: 还需要运行 {num_init_points_to_run} 个初始点。 ---\")\n            for i in range(num_init_points_to_run):\n                print(f\"\\n--- 初始点 {num_existing_points + i + 1}/{self.init_points} ---\")\n                x_new = self.random_sample()\n                y_new = objective_fn(x_new)\n                self.X_obs = np.vstack([self.X_obs, x_new])\n                self.y_obs = np.append(self.y_obs, y_new)\n                self._write_log(iteration=num_existing_points + i + 1, x_new=x_new, y_new=y_new)\n        \n        elif num_existing_points > 0:\n            # 场景B: 成功恢复运行。加载的点数已满足要求，跳过随机搜索。\n            print(f\"--- 成功恢复优化: 已加载 {num_existing_points} 个点。跳过 {self.init_points} 个初始点的随机搜索阶段。 ---\")\n        \n        else:\n            # 场景C: 全新运行 (没有加载任何数据)\n            print(f\"--- 开始全新运行: 执行 {self.init_points} 个初始随机点 ---\")\n            for i in range(self.init_points):\n                print(f\"\\n--- 初始点 {i + 1}/{self.init_points} ---\")\n                x_new = self.random_sample()\n                y_new = objective_fn(x_new)\n                self.X_obs = np.vstack([self.X_obs, x_new])\n                self.y_obs = np.append(self.y_obs, y_new)\n                self._write_log(iteration=i + 1, x_new=x_new, y_new=y_new)\n\n        print(f\"\\n--- 开始执行 {n_iter} 次新的贝叶斯优化迭代 ---\")\n\n        \n        for i in range(n_iter):\n            print(f\"\\n--- 优化迭代 {i+1}/{n_iter} ---\")\n            \n            theta_samples = self.sample_theta_posterior()\n            \n            best_ei = -1\n            best_x = None\n            for _ in range(200): # 增加随机搜索次数以更好地优化采集函数\n                x_candidate = self.random_sample()\n                ei = self.expected_improvement(x_candidate, theta_samples)\n                if ei > best_ei:\n                    best_ei = ei\n                    best_x = x_candidate\n            \n            if best_x is None:\n                print(\"警告：无法找到有效的候选点，将进行随机采样。\")\n                best_x = self.random_sample()\n\n            y_new = objective_fn(best_x)\n            \n            self.X_obs = np.vstack([self.X_obs, best_x])\n            self.y_obs = np.append(self.y_obs, y_new)\n            # 在每次优化后也写入日志 ---\n            self._write_log(iteration=self.init_points + i + 1, x_new=best_x, y_new=y_new)\n\n\n        best_idx = np.argmax(self.y_obs)\n        return self.X_obs[best_idx], self.y_obs[best_idx]\n\n   \n            \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport gc \n#---------------------------- main---------------------------------\n\nsearch_space = [\n    (1e-5, 5e-4),    # lr\n    (64, 512),       # dense_units\n    (0.2, 0.7),      # dropout_rate\n    (1e-5, 5e-3),    # l2_reg\n    (0, 3.99),      # 0=Adam, 1=SGD, 2=RMSprop, 3=Adagrad\n    (0.85, 0.99)     # momentum\n]\n#=============================================================================================\ndef my_objective_fn(x_new):\n    # 将优化器给出的numpy数组转换为超参数字典\n    hparams = {\n        'lr': x_new[0],\n        'dense_units': int(x_new[1]),\n        'dropout_rate': x_new[2],\n        'l2_reg': x_new[3],\n        'optimizer_choice': int(x_new[4]),\n        'momentum': x_new[5],\n    }\n    # 清理会话并调用训练函数\n    tf.keras.backend.clear_session()\n    gc.collect()\n    print(f\"\\n正在评估超参数: {hparams}\")\n    accuracy = train_and_evaluate_model(hparams, train_ds, val_ds, epochs=40)\n    print(f\"评估完成 - 验证集准确率: {accuracy:.4f}\")\n    return accuracy\n\n# 4.4 实例化并运行优化器\noptimizer = BayesianOptimizer(search_space=search_space, \n                              init_points=10,\n                              log_file=\"./EI_MCMC_BO_log.json\" \n                             )\n\nbest_x, best_y = optimizer.optimize(objective_fn=my_objective_fn, n_iter=15)  \n#===============================================================================================\n\nprint(\"\\n\" + \"=\"*50); print(\"贝叶斯优化完成!\")\nprint(f\"最佳验证准确率: {best_y:.4f}\")\n\nparam_names = ['lr', 'dense_units', 'dropout_rate', 'l2_reg', 'optimizer_choice', 'momentum']\nbest_hparams = {name: val for name, val in zip(param_names, best_x)}\n# 修正类型\nbest_hparams['dense_units'] = int(best_hparams['dense_units'])\nbest_hparams['optimizer_choice'] = int(best_hparams['optimizer_choice'])\n\nprint(f\"最佳超参数: {best_hparams}\")\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 贝叶斯优化全过程的图像表示\n# ===================================================================\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.set_style('whitegrid')\n\nlog_file_path = \"EI_MCMC_BO_log.json\" \n\ninitial_points = 10\n\ntry:\n    with open(log_file_path, 'r') as f:\n        logs = [json.loads(line) for line in f]\n    \n    df_results = pd.DataFrame(logs)\n    all_accuracies = df_results['accuracy'].values\n    print(f\"✅ Log file '{log_file_path}' loaded successfully! Loaded {len(all_accuracies)} evaluation records.\")\n    \n    \n    if len(all_accuracies) > initial_points:\n        accuracies_in_init_phase = all_accuracies[:initial_points]\n        best_random_accuracy = np.max(accuracies_in_init_phase)\n        best_final_accuracy = np.max(all_accuracies)\n        absolute_improvement = best_final_accuracy - best_random_accuracy\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"最终优化数据:\")\n        print(\"=\"*50)\n        print(f\"- 全局最优Val Accuracy:                 {best_final_accuracy:.4f}\")\n        print(f\"- 探索阶段的最优Val Accuracy :  {best_random_accuracy:.4f}\")\n        \n        if best_random_accuracy > 0:\n            relative_improvement = (absolute_improvement / best_random_accuracy) * 100\n            print(f\"- Absolute Improvement from Bayesian Opt: +{absolute_improvement:.4f} points\")\n            print(f\"- Relative Improvement from Bayesian Opt: +{relative_improvement:.2f}%\")\n        else:\n            print(f\"- Absolute Improvement from Bayesian Opt: +{absolute_improvement:.4f} points\")\n        print(\"=\"*50)\n\n    else:\n        print(\"\\nInsufficient evaluations to calculate Bayesian optimization improvement.\")\n\n   \n    n_evaluations = len(all_accuracies)\n    x_axis = np.arange(1, n_evaluations + 1)\n    best_idx = np.argmax(all_accuracies)\n    best_x = best_idx + 1\n    best_y = all_accuracies[best_idx]\n    plt.figure(figsize=(14, 8))\n  \n    plt.plot(x_axis, all_accuracies, 'o-', label='Evaluation Accuracy', color='steelblue', alpha=0.7)\n\n    plt.plot(x_axis, np.maximum.accumulate(all_accuracies), 'r--', label='Progressive Best Accuracy', linewidth=2.5)\n\n    plt.plot(best_x, best_y, \n             marker='*', \n             color='gold', \n             markersize=25, \n             markeredgecolor='black',\n             linestyle='', \n             label=f'Global Best Point (Accuracy: {best_y:.4f})\\nat Evaluation #{best_x}')\n\n   \n    plt.axvline(initial_points, color='gray', linestyle=':', linewidth=2, label=f'End of {initial_points} Random Explorations')\n\n   \n    plt.xlabel('Number of Evaluations', fontsize=12)\n    plt.ylabel('Validation Accuracy', fontsize=12)\n    plt.title(f'Bayesian Optimization Progress Curve (Overall Best Accuracy: {best_final_accuracy:.4f})', fontsize=16)\n    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n    plt.legend(fontsize=12)\n    plt.tight_layout()\n\n    \n    plt.savefig('final_optimization_curve_en.png')\n    plt.show()\n\nexcept FileNotFoundError:\n    print(f\"Error: Log file '{log_file_path}' not found. Please ensure the path is correct.\")\nexcept Exception as e:\n    print(f\"An error occurred during processing or plotting: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T10:58:08.049283Z","iopub.execute_input":"2025-09-04T10:58:08.049595Z","iopub.status.idle":"2025-09-04T10:58:08.062665Z","shell.execute_reply.started":"2025-09-04T10:58:08.049571Z","shell.execute_reply":"2025-09-04T10:58:08.061882Z"}},"outputs":[],"execution_count":null}]}